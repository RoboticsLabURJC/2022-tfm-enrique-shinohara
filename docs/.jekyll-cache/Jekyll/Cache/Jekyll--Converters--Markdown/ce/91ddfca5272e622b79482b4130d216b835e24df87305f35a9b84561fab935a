I"@<p>This week comes as a good point of inflection because by understanding more about the PilotNet and how it works, it was possible to solve a big chunk of the problems I had this past few weeks. But once again, before we see the results right away, it would nice to check what were the changes that led us to finding a good solution.</p>

<h2 id="the-model-arquitecture">The model arquitecture</h2>

<p>Being one big part of the whole deep learning project, the model was one of the few things we didn’t quite tweak to solve our little problem. This little problem consisted of how we translated the output to the Carla Simulation, this is because the output always gave us values bigger than one. This by itself was the main thing that needed fixing, we weren’t dealing correctly with how the model trained from the input data to the output values. So, once we had that checked, it was the moment to try different configurations with the PilotNet model, and the holy grail of the solution was: normalization.</p>

<p>But by normalizing the test image, we had a model with a pretty big systematic offset between the predictions and the groundtruth that obviously wasn’t learning at all. The next image is an example of the previous model and its prediction over the normalized test image, it showed me that it was certainly learning something, but didn’t correlate with the groundtruth scope of the steering values. So, being quite sure that the normalization was part of the problem, we decided to tamper with the arquitecture of the model.</p>

<figure class="align-center" style="width:100%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/plot_graph_epoch0_prediction_normalized.png" alt="" />
  <figcaption>Graph of the predicted steering values (red) against the groundtruth (green).</figcaption>
</figure>

<p>The normalization part of this whole project was already covered firstly by a BatchNormalization introduced in the first layer of the model, and secondly on the augmentations of the input images. But it resulted to be insufficient for our problem, by having so many parameters from the convolutional layer passed on to the dense layers, it was taken as an idea to scale this parameters by using a normalization layer and once this was added, we checked wether the model was learning and was staying within the expected range of 0-1.</p>

<iframe src="https://giphy.com/embed/GeY8aqy8gF3zPcwGH5" width="600" height="400" frameborder="0" class="align-center" allowfullscreen=""></iframe>
<p></p>

<p>And as we can see from the previous graph, it was! The model was adjusting itself from within the correct range and learning the good weights to adjust to the groundtruth.</p>

<h2 id="training">Training</h2>

<p>The training part stays pretty much the same as the previous week, for we were doing it correctly, not knowing that the solution wasn’t on the dataset itself or the training parameters but on the model itself. By balancing the dataset as we did and training for 120 epochs, finally we found an almost good configuration for the follow road task we had for the Carla Simulator.</p>

<figure class="align-center" style="width:70%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/balanced_235.png" alt="" />
  <figcaption>Histogram of the balanced dataset</figcaption>
</figure>

<figure class="align-center" style="width:70%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/epoch_loss_235_good.png" alt="" />
  <figcaption>Graph of the loss value along 120 epochs</figcaption>
</figure>

<p>One thing to notice is that after 40 epochs, the loss stabilize, making it good to stop the training earlier instead of running it for 120 epochs.</p>

<h2 id="results">Results</h2>

<p>Finally, the results are pretty much self-explanatory, where the first video shows us an easier example were we teached the car to turn correctly a single turn being the approach from either left or right.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/3DyRwm_xttw"></iframe></a>
</figure>

<p>In this second video, the car will run through three different maps. One of them will be a known map (Town05), a map used as a dataset for training the car, and the other two towns are maps that the car never saw before, for they weren’t used for the training and validation. We can see that it performs really well as expected, noticing that it turns correctly on sharp curves and on smoother curves.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/7jq29fJP9e4"></iframe></a>
</figure>
:ET
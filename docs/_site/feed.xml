<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/2022-tfm-enrique-shinohara/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/2022-tfm-enrique-shinohara/" rel="alternate" type="text/html" /><updated>2022-09-29T10:10:53+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/feed.xml</id><title type="html">Robotics Lab URJC</title><subtitle>Programming Robot Intelligence</subtitle><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><entry><title type="html">Week 15 - Building momentum</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-15/" rel="alternate" type="text/html" title="Week 15 - Building momentum" /><published>2022-09-28T00:00:00+02:00</published><updated>2022-09-28T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-15</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-15/"><![CDATA[<p>Finally, we have made some visible progress with the car learning how to follow the lane implicitly. To do this the idea is pretty much the same as the previous weeks, to enrich the dataset with more complex situations so that the car is able to handle the straights and the curves of the road. Once we have increased once again the dataset, we have the next composition of a balanced dataset.</p>

<figure style="width:80%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/histogram_20+curves+weird_extreme_good.png" alt="" />
  <figcaption>Histogram of the steering values used for training</figcaption>
</figure>

<p>In total we end up with around 45.000 images. By adding this complex situations and adjusting the output of the network, this last task has to be done manually, adjusting the range of the output with the range of the Carla simulation to be able to interpret the predictions of the neural network. Once we have all this adjusted, we end up with the next run.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/YDHKA6ymo7s"></iframe></a>
</figure>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/RAjDoJGylo4"></iframe></a>
</figure>

<p>It is interesting to watch some of the behaviour of the car in the previous video, for it has learned how to correct complex scenarios. As an example, in the first video (around the 28 second) once the turn is over, the car heads for the sidewalk, but then abruptly turns to the left straightening itself back to the lane. Trying to remember, previously we added to the dataset some weird start cases were we turned the car some degrees along the yaw so that it could learn how to straight itself back to the lane, and in the example from the video we can appreciate similar behaviour were it tries to overcome this scenario by following the lane accordingly.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><summary type="html"><![CDATA[Finally, we have made some visible progress with the car learning how to follow the lane implicitly. To do this the idea is pretty much the same as the previous weeks, to enrich the dataset with more complex situations so that the car is able to handle the straights and the curves of the road. Once we have increased once again the dataset, we have the next composition of a balanced dataset.]]></summary></entry><entry><title type="html">Week 14 - Why is it not turning?</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-14/" rel="alternate" type="text/html" title="Week 14 - Why is it not turning?" /><published>2022-09-17T00:00:00+02:00</published><updated>2022-09-17T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-14</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-14/"><![CDATA[<p>Carry on with the task at hand, we still need to figure out why the car is not turning when we increased the dataset with more turning situations. Wasn’t the quantity of increased data enough? More tweaking on the augmentation to see which turning value needs more weight? Even more variability on the dataset?</p>

<p>To tackle this questions and find the solution, an enumeration of where proposed for me by my tutor and advisor. The lines to keep on track right now are:</p>

<ul>
  <li>Introduce more variety onto the dataset</li>
  <li>Measure the behaviour of the network</li>
  <li>Explore other works along the side of a car following a road using neural networks</li>
</ul>

<p>To conclude the exploration part of this week, it is possible to find blogs of people who have tried this problem before with success. As expected, this is just the base when we see the bigger picture (given that we have also in mind to experiment with reinforcement learning), but is a base that we need to “conquer”. Given that this problem is being draged on since the previous week, we need to establish the next route to find the problem, and as one of the tasks at hand, we are going to need to search and measure the neural network and its training process (where the problem will be more likely to be found).</p>

<h2 id="measuring-the-behaviour-of-the-training-process-and-increasing-the-dataset">Measuring the behaviour of the training process and increasing the dataset</h2>

<p>As an experiment, the training was made with 45 epochs, to show us the behaviour of the training with our dataset and see how many epochs would be necessary for our model to not overfit. With this little experiment we simply visualize better that the training stabilize around 10 epochs.</p>

<figure style="width:80%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/loss_45.png" alt="" />
  <figcaption>Evolution of the error along 45 epochs.</figcaption>
</figure>

<p>One of the possible explanation of why is the car not turning, could be that the car is overfitting on straights, meaning that it is not extracting good, or enough information from the curves. To solve this, we have tried to increase the data, one by adding more curve cases to our dataset, and second by adding weird start cases. This last one has the purpose of giving the car more context of what to do in this strange situations. To do this, we simply rotate the yaw of the car along different points of the road for it to adjust itself and continue on the road as the follow-road algorithm intend it to.</p>

<iframe src="https://giphy.com/embed/VUcpEtOCDNYeebZLcO" width="480" height="360" frameborder="0" class="align-center" allowfullscreen=""></iframe>
<p></p>

<p>By making a scatter plot of the groundtruth and the predicted values of the steering angle, we can have a good understanding of what is happening with the neural network. One of the first thing we can notice, is a certain correlation between this two, meaning that it is certainly learning something. On the bad side, we can comment on two things: one, the visible vertical line in the middle, meaning that the car still oscillates to maintain itself inside the road, otherwise it would learn to follow the road more smoothly, and second, the Y-axis of the plot. This last part is quite tricky, because it means that the network is doing some predictions that don’t correspond to the range of values from which it was trained.</p>

<p>To better understand this, the dataset saved for the training has two values, the steering and the throttle. And while the throttle range from 0 to 1 in the Carla Simulation, the steering range from -1 to 1. This was the way the dataset was saved, but one solution that helped to understand this better is the normalization of the neural network. Normally, the neural networks train better when the data is normalized between 0 and 1, so by changig this step we only need to denormalize the values predicted from the model.</p>

<p>The normalization of the data gives us faster and probably an overall better training of our model, but this doesn’t takes the fact that the output values that the model is giving us doesn’t correspond with the range of values from the input. One solution that helped to improve the results were to try to interpret better how the ouput of the network correlates with the -1 to 1 range of the Carla simulation steering. Once this is tuned correctly, the car shows us a better take on how it handle the curve.</p>

<p>The two videos below shows us how the car performs on two different curves. They both approach it a little differently but both of them show us more improvement on the overall car behaviour.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/ItRGSR-uukg"></iframe></a>
</figure>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/mY9dlCyR_os"></iframe></a>
</figure>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="scatter plot" /><category term="augmentation" /><summary type="html"><![CDATA[Carry on with the task at hand, we still need to figure out why the car is not turning when we increased the dataset with more turning situations. Wasn’t the quantity of increased data enough? More tweaking on the augmentation to see which turning value needs more weight? Even more variability on the dataset?]]></summary></entry><entry><title type="html">Week 13 - Handling the curve</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-13/" rel="alternate" type="text/html" title="Week 13 - Handling the curve" /><published>2022-09-14T00:00:00+02:00</published><updated>2022-09-14T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-13</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-13/"><![CDATA[<p>Stumbling myself with an obstacle, in this week, the work was mainly focused on improving the dataset to handle better the curves. We already had a car that was able to follow the road when it was straight, but as soon as a curve was in front of it, it began to fail. The solution for this problem was discused in the previous post, having a dataset mainly composed of straight roads, the car didn’t have much information on how to handle other cases like curves. So, the solution we took was to increase it, by “recording” only the cases where the car was on a curve we are going to have more data for this cases.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/histogram_13+curves.png" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/histogram_13+curves_extreme.png" alt="" />
  <figcaption>Histogram of the original dataset (left) and the oversampled dataset (right) of the steering values.</figcaption>
</figure>

<p>The training was increased to 10 epochs. The next image shows us better, the stabilization of the error on the validation dataset.</p>

<figure style="width:80%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/epoch_loss_1.png" alt="" />
  <figcaption>Evolution of the error along 10 epochs.</figcaption>
</figure>

<p>The model obtained from this configuration, gives us a car that is able to follow the straight line pretty smoothly without the oscilation we had on the previous model, but it still not capable of taking the curve correctly.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="pilotnet" /><category term="dataset" /><summary type="html"><![CDATA[Stumbling myself with an obstacle, in this week, the work was mainly focused on improving the dataset to handle better the curves. We already had a car that was able to follow the road when it was straight, but as soon as a curve was in front of it, it began to fail. The solution for this problem was discused in the previous post, having a dataset mainly composed of straight roads, the car didn’t have much information on how to handle other cases like curves. So, the solution we took was to increase it, by “recording” only the cases where the car was on a curve we are going to have more data for this cases.]]></summary></entry><entry><title type="html">Week 12 - Following the road</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-12/" rel="alternate" type="text/html" title="Week 12 - Following the road" /><published>2022-09-07T00:00:00+02:00</published><updated>2022-09-07T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-12</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-12/"><![CDATA[<p>With this first week after the summer vacation, I return to the routine with a similar task to the one I had at hand on the summer. Instead of training the car to follow a line, we are going to put our focus on training a car to follow a lane, bringing it closer to a real life problem with the autonomous driving task.</p>

<p>For this, we need to replicate the steps we took on the follow line problem so that we can continue improving the model and analyzing possible improvements. So, to recall, we have 3 major tasks to tackle:</p>

<ul>
  <li>Create an explicit brain for the follow lane problem</li>
  <li>Create a dataset from the explicit brain algorithm</li>
  <li>Train with our dataset, a PilotNet model</li>
</ul>

<h2 id="creating-the-explicit-follow-lane-brain">Creating the explicit follow-lane brain</h2>

<p>The follow lane brain was already made as seen on the Week 3 post, but building upon this algorithm, I have improve it to take better the curves and to oscillate less when the left lane dissapears for a moment in an intersection.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/P9wC4w6Jy8E"></iframe></a>
</figure>

<h2 id="creating-the-dataset-for-the-follow-lane">Creating the dataset for the follow lane</h2>

<p>The creation of the dataset follows the same lines as the one we took on the previous post. The dataset contains the images, the steering and the throttle values on the timestep where the image was taken. A total of 5Gb of data was collected giving us an overall of almost 25.000 images. Differences when compared to the dataset on the previous blog resides on the task which was to follow a line, the algorithm created this time to follow the road is more consistent, meaning that the car almost never oscilate, giving us most of the data when the steer is not moving or is moving a negligible amount, and little to none data when it is steering left or right.</p>

<p>By getting the steering values on an histogram we can have a better view of the data from another perspective. In the next image, we can appreciate the imbalance of the data.</p>

<figure style="width:80%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/histogram_20.png" alt="" />
  <figcaption>Histogram of the steering values in our dataset.</figcaption>
</figure>

<p>By doing some oversampling, we can try to use better the data, giving more weight to the “extreme” cases that are almost never considered when compared to the straight steering values.</p>

<figure style="width:80%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/histogram_20_extreme.png" alt="" />
  <figcaption>Histogram of the oversampled steering values in our dataset.</figcaption>
</figure>

<h2 id="training-the-pilotnet-model">Training the PilotNet model</h2>

<p>The training of a PilotNet-based model is pretty straightforward. As explained in the previous posts, we are going to use the Tensorflow library to train the model. Bellow, you can see the video of the car behavior.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/llVOrcsoLvM"></iframe></a>
</figure>

<p>As we can check from the video, the car performs well on the straight road, correcting itself in order to stay on the center of the lane. But as soon as it gets to the curve, it doesn’t seems to find the correct way to turn itself. A further research is going to be needed for us to find why the car is not performing as we want it to on the curves, and how to correct it.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="pilotnet" /><category term="dataset" /><summary type="html"><![CDATA[With this first week after the summer vacation, I return to the routine with a similar task to the one I had at hand on the summer. Instead of training the car to follow a line, we are going to put our focus on training a car to follow a lane, bringing it closer to a real life problem with the autonomous driving task.]]></summary></entry><entry><title type="html">Week 7~11 - Slow but steady wins the race</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-7-11/" rel="alternate" type="text/html" title="Week 7~11 - Slow but steady wins the race" /><published>2022-08-06T00:00:00+02:00</published><updated>2022-08-06T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-7-11</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-7-11/"><![CDATA[<p>As we enter a vacation period in our thesis planification, advances will surely slow down when we compared them with the previous weeks. But this does not mean that no progress will be made in the meantime. First, for the sake of organizing the month we need to know what our to-do list is going to be, meaning, what is the primary problem to be solved right now.</p>

<p>As we established in the sixth week, we need to refine even more the brain we are using to make our car, in the Carla Simulator, follow a line. We already made some improvements in the last week but they weren’t enough, for the car wasn’t technically following the line as the model was trained to do. In order to find where the problem might be, we tried to simulate pretty closely the <a href="https://github.com/JdeRobot/DeepLearningStudio">DeepLearningStudio</a> project (even though this one was trained on the Gazebo Simulator), that is, the image we feeded to the model was a close representation of the images from which they trained their models, and the outputs were scaled to try to translate the Gazebo values to Carla values. Because this changes didn’t make the car follow the line as we wanted it to, an assumption was made by thinking that the problem resided in the differences between the both simulations cars dynamics. To pinpoint wether this was true or not, we have to test it by training a model using a Carla Simulation based dataset.</p>

<p>Now that we have landed into the tasks at hand, the next steps can be divided in:</p>
<ul>
  <li>Create an explicit brain to make our car follow a line. We already had a brain to make the car follow the road, but for the sake of undestanding the problem we explained beforehand, we need to simplify the task, by making it follow a line.</li>
  <li>Create a dataset for the follow-line task in the Carla Simulation. This dataset will be recolected with the use of the explicit follow-line brain.</li>
  <li>Train a model on our own with the recently created dataset, to see if it is capable of following the line better than the PilotNet-based model trained in Gazebo.</li>
</ul>

<h2 id="creating-the-explicit-follow-line-brain">Creating the explicit follow-line brain</h2>

<p>For this first task, we are going to make our car follow a line, in our case, this line will be the lane that separates the road. In order to solve this problem, we are goint to use what I learned from the Robotic Vision course in my master’s studies, in which luckily enough we were asked to keep a <a href="https://enriqueshino.wixsite.com/visionrobotica/post/follow-line">blog</a> to track our progress for the different projects. The project that interests us is one where we were asked to make a Formula 1 car follow a line. Pretty much the same, right?</p>

<p>Few changes were made to the controller, for we don’t need to make our car follow the line as fast as possible. One change that we can notice is that our line in the Carla Simulator is not exactly a single line, but this have a rather simple solution for we only need to get the left side of the left line and the right side of the right line to get the center of this two.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/ecg7oopKsJc"></iframe></a>
</figure>

<h2 id="creating-and-organizing-the-data">Creating and organizing the data</h2>

<p>Now that we have a car that follows a line, we need to collect data from multiple runs. The data we are going to collect is pretty straightforward: the image from the frontal camera, the throttle and the steering value taken from the moment the image was taken.</p>

<figure style="width:60%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/dataset_carla.png" alt="" />
  <figcaption>Number of instances taken for our dataset.</figcaption>
</figure>

<p>This gets us to a total of a little more than 15.000 images from the dataset. As always, analyzing the quality of the data is an important step for machine learning in order to understand better if our neural network is going to learn like we want it to, or if we are feeding it some uncomplete data.</p>

<figure style="width:80%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/histogram_steer_12.png" alt="" />
  <figcaption>Histogram of the steering values in our dataset.</figcaption>
</figure>

<p>We are going to need to polish our dataset, meaning, that we need a better distributed dataset if we are going to make our car follow a line correctly. To improve the dataset we have two options that comes right now to mind. One is increasing our dataset, and the other one is oversampling. The thing with increasing our dataset is that we are still going to need to balance the data, for the variability in our dataset recollections is pretty low. So, by oversampling the weird cases in the dataset we are able to increase the weight of its value in the process of training.</p>

<figure style="width:80%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/histogram_steer_12_extreme.png" alt="" />
  <figcaption>Histogram of the steering values in our balanced dataset.</figcaption>
</figure>

<h2 id="training-and-results">Training and results</h2>

<p>With our new balanced dataset, we use the tensorflow library to train a PilotNet model to follow a line. The behaviour of the model can be seen in the next video.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/IslVARKBdn4"></iframe></a>
</figure>

<p>Concluding with the last video, we have created an explicit follow line algorithm, organize and created a dataset and trained a model (such as the PilotNet) to implement onto the car, and in doing so we have established that training the model on the Carla Simulation dataset, improve the behaviour of the car on the task of following a line. But we may need to be able to translate dataset from other simulations (even from real data) if we want to train from them our Carla vehicle. That being said, we have made some interesting progress towards having machine learning methods used on the Carla environment which is pretty cool :)</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="tensorflow" /><category term="dataset" /><summary type="html"><![CDATA[As we enter a vacation period in our thesis planification, advances will surely slow down when we compared them with the previous weeks. But this does not mean that no progress will be made in the meantime. First, for the sake of organizing the month we need to know what our to-do list is going to be, meaning, what is the primary problem to be solved right now.]]></summary></entry><entry><title type="html">Week 6 - Refining some code and testing some frequencies among others…</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-6/" rel="alternate" type="text/html" title="Week 6 - Refining some code and testing some frequencies among others…" /><published>2022-07-27T00:00:00+02:00</published><updated>2022-07-27T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-6</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-6/"><![CDATA[<p>For this week we have multiple work borders open that I am going to explain more in detail in this post. To have a better visual of it, the task were as follows:</p>

<ul>
  <li>Keep exploring some datasets to train from</li>
  <li>Study of the frames-per-second and frequencies from the server-client model simulation, Carla</li>
  <li>Be able to train a model in my local computer from a <a href="https://github.com/JdeRobot/DeepLearningStudio/tree/main/Formula1-FollowLine/tensorflow">dataset</a> made by <a href="https://sergiopaniego.github.io/">Sergio Paniego</a></li>
  <li>Refining the previous PilotNet model implemented in a vehicle from the Carla Simulator</li>
</ul>

<h2 id="dataset-exploration">Dataset exploration</h2>

<p>First, the exploration of the data, a task that I found more complicated than I thought, for most of the public dataset for self-driving cars were orientated towards segmentation and detection tasks. Despite these complications I could find multiple <a href="https://academictorrents.com/userdetails.php?id=5125">datasets</a> from Udacity, an educational organization which has available multiple dataset for free, that comes with the images from RGB cameras, the steering and the throttle values among others that I find most useful for future task of my thesis.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/udacity_1.jpg" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/udacity_2.jpg" alt="" />
  <figcaption>Examples images from the udacity dataset.</figcaption>
</figure>

<h2 id="frequency-study">Frequency study</h2>

<p>Secondly I centered myself most of the week on trying to understand and comprehend, the necessary information that comes from analyzing the frequencies on the server-client simulation, Carla. To better understand how our machine can handle the simulation demand of resources, we need to know if the speed at which the server is rendering new images is fast enought to keep with the client speed to recieve and process the images, and viceversa. In order to do this, we analyze the frequencies in two possible ways, one where the client only recieves the images, and other where the client uses a model to predict the images output. A table containing the frames-per-second from the client and the server without using a PilotNet model and using it can be seen below.</p>

<table style="border-collapse:collapse;border-spacing:0" class="tg"><thead><tr><th style="background-color:#656565;border-color:#000000;border-style:solid;border-width:1px;color:#ffffff;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">FPS</th><th style="background-color:#656565;border-color:#000000;border-style:solid;border-width:1px;color:#ffffff;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" colspan="2">Client</th><th style="background-color:#656565;border-color:black;border-style:solid;border-width:1px;color:#ffffff;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Server</th></tr></thead><tbody><tr><td style="background-color:#656565;border-color:#000000;border-style:solid;border-width:1px;color:#ffffff;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Empty</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" colspan="2">5*10⁵</td><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">87</td></tr><tr><td style="background-color:#656565;border-color:#000000;border-style:solid;border-width:1px;color:#ffffff;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Model<br /></td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" colspan="2">23</td><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">60 - 80</td></tr></tbody></table>

<p>Further study of this results may be needed, for the frames-per-second were seen decaying the longer we kept the simulation running. Also, the results from this version of the code were different compared to an older version of the code, so I would like to deepen my understanding of this little inconsistency.</p>

<h2 id="model-training">Model training</h2>

<p>A training of a model from the <a href="https://github.com/JdeRobot/DeepLearningStudio">DeepLearningStudio</a> repository was achieved, by adjusting the Python version to 3.7 and 3.6 along with some minor updates for the packages. It was posible to train a model in our local machine using the DeepLearningStudio dataset. It took around 7 minutes to complete the training. More study of the code would be wise to again deepen my knowledge of how to use Tensorflow to train deep neural networks.</p>

<h2 id="refining-the-pilotnet-based-model-for-carla">Refining the PilotNet-based model for Carla</h2>

<p>For the final task of the week, we needed to test wether if we were adapting correctly the PilotNet-based model from <a href="https://github.com/JdeRobot/DeepLearningStudio">DeepLearningStudio</a> to the carla simulation or if we were doing something wrong as we saw in the previous post from the failed neural network video. To do this, two changes were made:</p>

<ul>
  <li>One of them consisted on cutting the horizon from the recieved image of the camera, and changing the color that delimitates the lanes to red. This changes were made in order to simulate in the closest way possible, the environment in which the model was trained.</li>
  <li>The other big change was basically to tweak the output (throttle and steer) of the model, to adapt it for the Carla Simulation values.</li>
</ul>

<p>The result of this refining work can be seen in the next video:</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/hZHqT77YTVU"></iframe></a>
</figure>

<p>Simulating the same path were we saw the model fail in the previous week, we can see that the car follow the lane without doing some weird movements that take it out of its course. It is clear, that the model was going to fail in our case when no lane is detected in the image, but this was the expected behaviour, because the vehicle is also following the lane as the model was train to do.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="udacity" /><category term="machine learning" /><summary type="html"><![CDATA[For this week we have multiple work borders open that I am going to explain more in detail in this post. To have a better visual of it, the task were as follows:]]></summary></entry><entry><title type="html">Week 5 - Setting up CUDA for Deep Neural Networks</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-5/" rel="alternate" type="text/html" title="Week 5 - Setting up CUDA for Deep Neural Networks" /><published>2022-07-20T00:00:00+02:00</published><updated>2022-07-20T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-5</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-5/"><![CDATA[<p>As we saw in the previous post, we are very close to finally playing with some machine learning algorithms for our road following car. But as always, first we need to prepare the playground before we dive deep into the machine learning problems, and by preparing I mean installing the ever famous <a href="https://developer.nvidia.com/cuda-toolkit">CUDA toolkit</a>, this tool will allow us to train and predict algorithms with GPU-acceleration by using the graphic card in our local machine. Given that I have a RTX 3060 6Gb graphics card in my laptop, it would be a total loss not to test it.</p>

<p>Having said that, installing CUDA for our purpose is not a simple task. The installation itself is pretty straight forward, but the environment in which we are going to use it, plays an important role when choosing the version of the software we are going to use it on. To do this, we must be thoughtful with four things that are going to be the ones that will cause the most problems if they are not compatible with each other:</p>

<ul>
  <li>CUDA Toolkit 11.0</li>
  <li>Python 3.6</li>
  <li>cuDNN 8.0</li>
  <li>Tensorflow 2.4.0</li>
</ul>

<p>The setting we finally choosed was defined firstly by the Carla Simulator, because we are using an old version of Carla 0.9.2 some libraries can only be used on a certain version of python, we have only tested them on 3.5 and 3.6. And secontly, Tensorflow, for it plays a huge rol on the selection of a certain CUDA and cuDNN version. On the <a href="https://www.tensorflow.org/install/source">Tensorflow installation</a> website, we can find a table with multiple tested configurations.</p>

<figure style="width:100%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/tensorflow_gpu.png" alt="" />
  <figcaption>Tensorflow GPU installation homepage.</figcaption>
</figure>

<p>Once we have our tensorflow supported by CUDA for high-optimized performance, the next step is to read, understand and test some code from the repository <a href="https://github.com/JdeRobot/DeepLearningStudio">DeepLearningStudio</a>. The respository contains code to train models (like a formula 1 car to follow a line) with deep learning which serves us as a guide of how to tackle this kind of problems. At first, the idea was to make a training session from the local machine without changing anything from the original code but because the recommended python version was the 3.10, this caused some compatibility issues with the latest CUDA versions from NVIDIA. To avoid stalling to much on this training part (I will deal with the training part later in the future), I decided to directly download some trained models and load them onto the code I had to make a car follow the road on the Carla Simulator.</p>

<p>With everything configurated it was time to configurate the model so that it recieves an image as input, and return as output two values, the velocity and the steering angle. Once it was coded, the execution showed us that the predictions made by the neural network took 5 seconds, and it was obvious something was off, because with the use of CUDA, even a second is too much for an autonomous vehicle application. By doing some changes in the Nvidia Driver and files inside the CUDA Toolkit, we were able to lower the prediction time to 0.2 seconds, and making a prediction every 10 frames we can make the simulation go smoother without deleting too much information of the actual state of the car.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/Rm1q7x_Pmxg"></iframe></a>
</figure>

<p>As we can see in the video it follows poorly the red line on the ground. In the next video, we can take a better look on how poorly is performing our car, for it is not even trying to follow the red line on the road.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/gpgRax2kqFk"></iframe></a>
</figure>

<p>Further testing will need to be made to check wether this behaviour is expected for our code or not.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="cuda" /><category term="tensorflow" /><summary type="html"><![CDATA[As we saw in the previous post, we are very close to finally playing with some machine learning algorithms for our road following car. But as always, first we need to prepare the playground before we dive deep into the machine learning problems, and by preparing I mean installing the ever famous CUDA toolkit, this tool will allow us to train and predict algorithms with GPU-acceleration by using the graphic card in our local machine. Given that I have a RTX 3060 6Gb graphics card in my laptop, it would be a total loss not to test it.]]></summary></entry><entry><title type="html">Week 4 - Neural Networks might come to action, but not just yet</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-4/" rel="alternate" type="text/html" title="Week 4 - Neural Networks might come to action, but not just yet" /><published>2022-07-13T00:00:00+02:00</published><updated>2022-07-13T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-4</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-4/"><![CDATA[<p>As we go further within the project, we stamble ourselfs already trying to introduce the concept of machine learning to the autonomous vehicle. But before we do this, it is necessary to find a good dataset from where we could begin training and testing some models. This step can be seen as a light introduction for later in the future where we will start to work with Reinforcement Learning techniques.</p>

<p>Seeing that the simulation we are using is Carla, it would be a good start to find some Carla related datasets for a vehicle driving autonomously following a road. But such specific dataset is difficult to find when we narrow down the datasets to only the ones from Carla simulator. As of now, we have found a <a href="https://github.com/SullyChen/driving-datasets">dataset</a> that gives us the image and the steering angle needed for making a vehicle to follow the road.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/2406.jpg" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/41121.jpg" alt="" />
  <figcaption>Examples images from the dataset.</figcaption>
</figure>

<p>Further research will be made in order to either search a better suited dataset for the objective at hand right now, or create our own dataset by driving the vehicle in Carla with the controller that we created on the third week.</p>

<p>One more task that we needed to tackle this week was either if we could retrieve more information from the simulation or not. Given that we can already extract RGB images from camera sensor, how many more sensors are available for us in the Carla Simulation. Because we are playing with a pretty old version of Carla, many usefull sensors are not yet available for us, but we can still have access to a LIDAR sensor and the location (x, y, z) of the car in the 3D world. By ignoring for now the simplicity to obtain the location of the car, we try to focus on the LIDAR, how to configure it and wether the adquisition of the cloud points is good or not.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/rgb_lidar.png" alt="" style="width:42%" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/lidar01.png" alt="" style="width:51%" />
  <figcaption>RGB image and LIDAR cloud points for the same frame in the simulation</figcaption>
</figure>

<p>Given my low understanding of how to treat LIDAR cloud points, I will need to experiment with it further more, but as of now, we have augmented the quantity of sensors that we can retrieve from the Carla Simulator.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="lidar" /><category term="sensors" /><category term="dataset" /><summary type="html"><![CDATA[As we go further within the project, we stamble ourselfs already trying to introduce the concept of machine learning to the autonomous vehicle. But before we do this, it is necessary to find a good dataset from where we could begin training and testing some models. This step can be seen as a light introduction for later in the future where we will start to work with Reinforcement Learning techniques.]]></summary></entry><entry><title type="html">Week 3 - Hello World</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-3/" rel="alternate" type="text/html" title="Week 3 - Hello World" /><published>2022-07-04T00:00:00+02:00</published><updated>2022-07-04T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-3</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-3/"><![CDATA[<p>As we make our way into the depths of the Carla Simulator, it is a good idea to grasp a better understanding of the Carla framework and its infrastructure. In order to do this, we need to start coding a “brain” for our car in the simulation, to be able to order commands and execute them on to the car, and as of now, our car needs two commands for the velocity: angular (steering angle) and linear (throttle or brake). For the sake of simplicity, the linear velocity will be constant, changing only the steering input.</p>

<p>Before we start with the coding section, it was important to play with the available towns/maps for the server. Doing a simple check on the maps, we can come up with the conclusion to stick with the Town02. This town is the easiest one roadwise, giving us also better performance on low graphics when compared to other towns. Below, we can have a better visual of the complexity of the roads on Town01, compared to Town02.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/town01_carla.png" alt="" style="width:51%" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/town02_carla.png" alt="" />
  <figcaption>Bird's-eye view of Town01 (left) and Town02 (right).</figcaption>
</figure>

<p>The objective right now is to implement a car to follow the road, keeping itself inside the lanes. In the case of intersections, the car will simply turn to the right, always. To do this, we want to hard code a simple program that with computer vision (using a single RGB camera) it would be able to extract the necessary information to follow the lane without getting out of it.
First we need to detect the lanes that surround our vehicle, to do this, the Canny edge detection algorithm can be really handy. By doing some morphological transformations to the image (dilation and erosion) we can highlight the lanes in our mask, making it possible to extract each lane coordinates in our image. Once we have this information, with some simple operations we can compute the direction of the “road”, that added to the middle position of our camera, it gives us the angle of the vehicle relative to the road.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/original.png" alt="" style="width:40%;height:5%" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/canny.png" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/morph.png" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/final.png" alt="" />
  <figcaption>Preprocessing sequence: Original (top-left), Canny (top-right), Morphological Transformation (bottom-left), Final (bottom-right).</figcaption>
</figure>

<p>Now, with the angle of the car known, we can make a proportional controller (which will be more than enough for the objective at hand right now). By adding the angle to the steering value, we can make proportional corrections and make it follow the road (green line on top of the red line). The rest of the implementation reside on doing some tweakings to the constant multiplied to the angle and consider the rare cases when the lane line disapear.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/1DdQwXonS0M"></iframe></a>
</figure>

<p>With this test, we have a better comprehension of how we can communicate with our vehicle in the Carla Simulator.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="computer vision" /><category term="opencv" /><summary type="html"><![CDATA[As we make our way into the depths of the Carla Simulator, it is a good idea to grasp a better understanding of the Carla framework and its infrastructure. In order to do this, we need to start coding a “brain” for our car in the simulation, to be able to order commands and execute them on to the car, and as of now, our car needs two commands for the velocity: angular (steering angle) and linear (throttle or brake). For the sake of simplicity, the linear velocity will be constant, changing only the steering input.]]></summary></entry><entry><title type="html">Week 2 - Testing Carla Simulation.</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-2/" rel="alternate" type="text/html" title="Week 2 - Testing Carla Simulation." /><published>2022-06-27T00:00:00+02:00</published><updated>2022-06-27T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-2</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-2/"><![CDATA[<p>A first introduction to the Carla simulation would be a good place to start with this weeks job, for we need to get acquainted with this simulation if we want to train a self autonomous vehicle in it.</p>

<figure style="width:70%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/carla.jpg" alt="" />
</figure>

<p>Carla, is an open-source simulator builded for autonomous driving research. The software behind Carla is constantly being mainteined and developed, having every year a few mayor launches, and forums where they are pretty active with the community. It was clearly a good starting point for the project in our hands, but being as good as it looks, Carla is computacionally very expensive, even more when sensors are added to the equation. This is why, it was important to try to do multiple tests so that we could come up with a solution, either if it could be possible to implement Reinforcement Learning algorithms in Carla, or if it would be better (for the future) to change to a lighter simulation.</p>

<p>Trying to make Carla work in my local device wasn’t going to be a simple task, given that the recommended virtual memory was 8Gb and my laptop has a 6Gb. The first idea was to install Carla 0.9.13, its latest version. Testing it, the results weren’t very promising, while the Client part of the simulator gives us 60 fps, the Server part was giving us 5 fps which is really bad, specially if we want to add sensors to the vehicle for reinforcement learning. This same behaviour was repeated for other versions of Carla above the 0.9.1X, so the idea now was to try for the older versions to see if the newer software was more demanding than the older ones, and finally with the 0.9.2 version we got jackpot, reaching 40+ fps on the server with lowered quality levels.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/-b8DpSdlsko"></iframe></a>
</figure>

<p>More tests will have to be made in the future to check whether the simulation is suitable for reinforcement learning or not but as of now, we have a stable working simulation that doesn’t drop the frames per second below 10.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="reinforcement learning" /><summary type="html"><![CDATA[A first introduction to the Carla simulation would be a good place to start with this weeks job, for we need to get acquainted with this simulation if we want to train a self autonomous vehicle in it.]]></summary></entry></feed>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/2022-tfm-enrique-shinohara/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/2022-tfm-enrique-shinohara/" rel="alternate" type="text/html" /><updated>2022-08-27T11:34:15+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/feed.xml</id><title type="html">Robotics Lab URJC</title><subtitle>Programming Robot Intelligence</subtitle><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><entry><title type="html">Week 7~11 - Slow but steady wins the race</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-7-11/" rel="alternate" type="text/html" title="Week 7~11 - Slow but steady wins the race" /><published>2022-08-06T00:00:00+02:00</published><updated>2022-08-06T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-7-11</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-7-11/"><![CDATA[<p>As we enter a vacation period in our thesis planification, advances will surely slow down when we compared them with the previous weeks. But this does not mean that no progress will be made in the meantime. First, for the sake of organizing the month we need to know what our to-do list is going to be, meaning, what is the primary problem to be solved right now.</p>

<p>As we established in the sixth week, we need to refine even more the brain we are using to make our car, in the Carla Simulator, follow a line. We already made some improvements in the last week but they weren’t enough, for the car wasn’t technically following the line as the model was trained to do. In order to find where the problem might be, we tried to simulate pretty closely the <a href="https://github.com/JdeRobot/DeepLearningStudio">DeepLearningStudio</a> project (even though this one was trained on the Gazebo Simulator), that is, the image we feeded to the model was a close representation of the images from which they trained their models, and the outputs were scaled to try to translate the Gazebo values to Carla values. Because this changes didn’t make the car follow the line as we wanted it to, an assumption was made by thinking that the problem resided in the differences between the both simulations cars dynamics. To pinpoint wether this was true or not, we have to test it by training a model using a Carla Simulation based dataset.</p>

<p>Now that we have landed into the tasks at hand, the next steps can be divided in:</p>
<ul>
  <li>Create an explicit brain to make our car follow a line. We already had a brain to make the car follow the road, but for the sake of undestanding the problem we explained beforehand, we need to simplify the task, by making it follow a line.</li>
  <li>Create a dataset for the follow-line task in the Carla Simulation. This dataset will be recolected with the use of the explicit follow-line brain.</li>
  <li>Train a model on our own with the recently created dataset, to see if it is capable of following the line better than the PilotNet-based model trained in Gazebo.</li>
</ul>

<h2 id="creating-the-explicit-follow-line-brain">Creating the explicit follow-line brain</h2>

<p>For this first task, we are going to make our car follow a line, in our case, this line will be the lane that separates the road. In order to solve this problem, we are goint to use what I learned from the Robotic Vision course in my master’s studies, in which luckily enough we were asked to keep a <a href="https://enriqueshino.wixsite.com/visionrobotica/post/follow-line">blog</a> to track our progress for the different projects. The project that interests us is one where we were asked to make a Formula 1 car follow a line. Pretty much the same, right?</p>

<p>Few changes were made to the controller, for we don’t need to make our car follow the line as fast as possible. One change that we can notice is that our line in the Carla Simulator is not exactly a single line, but this have a rather simple solution for we only need to get the left side of the left line and the right side of the right line to get the center of this two.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/ecg7oopKsJc"></iframe></a>
</figure>

<h2 id="creating-and-organizing-the-data">Creating and organizing the data</h2>

<p>Now that we have a car that follows a line, we need to collect data from multiple runs. The data we are going to collect is pretty simple: the image from the frontal camera, the throttle and the steering value at the moment the image was taken.</p>

<figure style="width:60%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/dataset_carla.png" alt="" />
  <figcaption>Number of instances taken for our dataset.</figcaption>
</figure>

<p>This gets us to a total of a little more than 15.000 images from the dataset. As always, analyzing the quality of the data is an important step for machine learning in order to understand better if our neural network is going to learn like we want it to, or if we are feeding it some uncomplete data.</p>

<figure style="width:80%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/histogram_steer_12.png" alt="" />
  <figcaption>Histogram of the steering values in our dataset.</figcaption>
</figure>

<p>For a first neural network, a first test is going to be made with this dataset. But, we are probably going to need to polish our dataset, meaning, we are going to need a better distributed dataset if we are going to make our car follow a line. Already, we can foresee the car performing probably well on straight lines, but badly on curves for the lack of turning values in the steering (exeption from the extreme cases -1 and 1).</p>

<h2 id="training-the-follow-line-model">Training the follow-line model</h2>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="gazebo" /><summary type="html"><![CDATA[As we enter a vacation period in our thesis planification, advances will surely slow down when we compared them with the previous weeks. But this does not mean that no progress will be made in the meantime. First, for the sake of organizing the month we need to know what our to-do list is going to be, meaning, what is the primary problem to be solved right now.]]></summary></entry><entry><title type="html">Week 6 - Refining some code and testing some frequencies among others…</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-6/" rel="alternate" type="text/html" title="Week 6 - Refining some code and testing some frequencies among others…" /><published>2022-07-27T00:00:00+02:00</published><updated>2022-07-27T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-6</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-6/"><![CDATA[<p>For this week we have multiple work borders open that I am going to explain more in detail in this post. To have a better visual of it, the task were as follows:</p>

<ul>
  <li>Keep exploring some datasets to train from</li>
  <li>Study of the frames-per-second and frequencies from the server-client model simulation, Carla</li>
  <li>Be able to train a model in my local computer from a <a href="https://github.com/JdeRobot/DeepLearningStudio/tree/main/Formula1-FollowLine/tensorflow">dataset</a> made by <a href="https://sergiopaniego.github.io/">Sergio Paniego</a></li>
  <li>Refining the previous PilotNet model implemented in a vehicle from the Carla Simulator</li>
</ul>

<h2 id="dataset-exploration">Dataset exploration</h2>

<p>First, the exploration of the data, a task that I found more complicated than I thought, for most of the public dataset for self-driving cars were orientated towards segmentation and detection tasks. Despite these complications I could find multiple <a href="https://academictorrents.com/userdetails.php?id=5125">datasets</a> from Udacity, an educational organization which has available multiple dataset for free, that comes with the images from RGB cameras, the steering and the throttle values among others that I find most useful for future task of my thesis.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/udacity_1.jpg" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/udacity_2.jpg" alt="" />
  <figcaption>Examples images from the udacity dataset.</figcaption>
</figure>

<h2 id="frequency-study">Frequency study</h2>

<p>Secondly I centered myself most of the week on trying to understand and comprehend, the necessary information that comes from analyzing the frequencies on the server-client simulation, Carla. To better understand how our machine can handle the simulation demand of resources, we need to know if the speed at which the server is rendering new images is fast enought to keep with the client speed to recieve and process the images, and viceversa. In order to do this, we analyze the frequencies in two possible ways, one where the client only recieves the images, and other where the client uses a model to predict the images output. A table containing the frames-per-second from the client and the server without using a PilotNet model and using it can be seen below.</p>

<table style="border-collapse:collapse;border-spacing:0" class="tg"><thead><tr><th style="background-color:#656565;border-color:#000000;border-style:solid;border-width:1px;color:#ffffff;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">FPS</th><th style="background-color:#656565;border-color:#000000;border-style:solid;border-width:1px;color:#ffffff;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" colspan="2">Client</th><th style="background-color:#656565;border-color:black;border-style:solid;border-width:1px;color:#ffffff;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Server</th></tr></thead><tbody><tr><td style="background-color:#656565;border-color:#000000;border-style:solid;border-width:1px;color:#ffffff;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Empty</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" colspan="2">5*10⁵</td><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">87</td></tr><tr><td style="background-color:#656565;border-color:#000000;border-style:solid;border-width:1px;color:#ffffff;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Model<br /></td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" colspan="2">23</td><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">60 - 80</td></tr></tbody></table>

<p>Further study of this results may be needed, for the frames-per-second were seen decaying the longer we kept the simulation running. Also, the results from this version of the code were different compared to an older version of the code, so I would like to deepen my understanding of this little inconsistency.</p>

<h2 id="model-training">Model training</h2>

<p>A training of a model from the <a href="https://github.com/JdeRobot/DeepLearningStudio">DeepLearningStudio</a> repository was achieved, by adjusting the Python version to 3.7 and 3.6 along with some minor updates for the packages. It was posible to train a model in our local machine using the DeepLearningStudio dataset. It took around 7 minutes to complete the training. More study of the code would be wise to again deepen my knowledge of how to use Tensorflow to train deep neural networks.</p>

<h2 id="refining-the-pilotnet-based-model-for-carla">Refining the PilotNet-based model for Carla</h2>

<p>For the final task of the week, we needed to test wether if we were adapting correctly the PilotNet-based model from <a href="https://github.com/JdeRobot/DeepLearningStudio">DeepLearningStudio</a> to the carla simulation or if we were doing something wrong as we saw in the previous post from the failed neural network video. To do this, two changes were made:</p>

<ul>
  <li>One of them consisted on cutting the horizon from the recieved image of the camera, and changing the color that delimitates the lanes to red. This changes were made in order to simulate in the closest way possible, the environment in which the model was trained.</li>
  <li>The other big change was basically to tweak the output (throttle and steer) of the model, to adapt it for the Carla Simulation values.</li>
</ul>

<p>The result of this refining work can be seen in the next video:</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/hZHqT77YTVU"></iframe></a>
</figure>

<p>Simulating the same path were we saw the model fail in the previous week, we can see that the car follow the lane without doing some weird movements that take it out of its course. It is clear, that the model was going to fail in our case when no lane is detected in the image, but this was the expected behaviour, because the vehicle is also following the lane as the model was train to do.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="udacity" /><category term="machine learning" /><summary type="html"><![CDATA[For this week we have multiple work borders open that I am going to explain more in detail in this post. To have a better visual of it, the task were as follows:]]></summary></entry><entry><title type="html">Week 5 - Setting up CUDA for Deep Neural Networks</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-5/" rel="alternate" type="text/html" title="Week 5 - Setting up CUDA for Deep Neural Networks" /><published>2022-07-20T00:00:00+02:00</published><updated>2022-07-20T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-5</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-5/"><![CDATA[<p>As we saw in the previous post, we are very close to finally playing with some machine learning algorithms for our road following car. But as always, first we need to prepare the playground before we dive deep into the machine learning problems, and by preparing I mean installing the ever famous <a href="https://developer.nvidia.com/cuda-toolkit">CUDA toolkit</a>, this tool will allow us to train and predict algorithms with GPU-acceleration by using the graphic card in our local machine. Given that I have a RTX 3060 6Gb graphics card in my laptop, it would be a total loss not to test it.</p>

<p>Having said that, installing CUDA for our purpose is not a simple task. The installation itself is pretty straight forward, but the environment in which we are going to use it, plays an important role when choosing the version of the software we are going to use it on. To do this, we must be thoughtful with four things that are going to be the ones that will cause the most problems if they are not compatible with each other:</p>

<ul>
  <li>CUDA Toolkit 11.0</li>
  <li>Python 3.6</li>
  <li>cuDNN 8.0</li>
  <li>Tensorflow 2.4.0</li>
</ul>

<p>The setting we finally choosed was defined firstly by the Carla Simulator, because we are using an old version of Carla 0.9.2 some libraries can only be used on a certain version of python, we have only tested them on 3.5 and 3.6. And secontly, Tensorflow, for it plays a huge rol on the selection of a certain CUDA and cuDNN version. On the <a href="https://www.tensorflow.org/install/source">Tensorflow installation</a> website, we can find a table with multiple tested configurations.</p>

<figure style="width:100%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/tensorflow_gpu.png" alt="" />
  <figcaption>Tensorflow GPU installation homepage.</figcaption>
</figure>

<p>Once we have our tensorflow supported by CUDA for high-optimized performance, the next step is to read, understand and test some code from the repository <a href="https://github.com/JdeRobot/DeepLearningStudio">DeepLearningStudio</a>. The respository contains code to train models (like a formula 1 car to follow a line) with deep learning which serves us as a guide of how to tackle this kind of problems. At first, the idea was to make a training session from the local machine without changing anything from the original code but because the recommended python version was the 3.10, this caused some compatibility issues with the latest CUDA versions from NVIDIA. To avoid stalling to much on this training part (I will deal with the training part later in the future), I decided to directly download some trained models and load them onto the code I had to make a car follow the road on the Carla Simulator.</p>

<p>With everything configurated it was time to configurate the model so that it recieves an image as input, and return as output two values, the velocity and the steering angle. Once it was coded, the execution showed us that the predictions made by the neural network took 5 seconds, and it was obvious something was off, because with the use of CUDA, even a second is too much for an autonomous vehicle application. By doing some changes in the Nvidia Driver and files inside the CUDA Toolkit, we were able to lower the prediction time to 0.2 seconds, and making a prediction every 10 frames we can make the simulation go smoother without deleting too much information of the actual state of the car.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/Rm1q7x_Pmxg"></iframe></a>
</figure>

<p>As we can see in the video it follows poorly the red line on the ground. In the next video, we can take a better look on how poorly is performing our car, for it is not even trying to follow the red line on the road.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/gpgRax2kqFk"></iframe></a>
</figure>

<p>Further testing will need to be made to check wether this behaviour is expected for our code or not.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="cuda" /><category term="tensorflow" /><summary type="html"><![CDATA[As we saw in the previous post, we are very close to finally playing with some machine learning algorithms for our road following car. But as always, first we need to prepare the playground before we dive deep into the machine learning problems, and by preparing I mean installing the ever famous CUDA toolkit, this tool will allow us to train and predict algorithms with GPU-acceleration by using the graphic card in our local machine. Given that I have a RTX 3060 6Gb graphics card in my laptop, it would be a total loss not to test it.]]></summary></entry><entry><title type="html">Week 4 - Neural Networks might come to action, but not just yet</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-4/" rel="alternate" type="text/html" title="Week 4 - Neural Networks might come to action, but not just yet" /><published>2022-07-13T00:00:00+02:00</published><updated>2022-07-13T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-4</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-4/"><![CDATA[<p>As we go further within the project, we stamble ourselfs already trying to introduce the concept of machine learning to the autonomous vehicle. But before we do this, it is necessary to find a good dataset from where we could begin training and testing some models. This step can be seen as a light introduction for later in the future where we will start to work with Reinforcement Learning techniques.</p>

<p>Seeing that the simulation we are using is Carla, it would be a good start to find some Carla related datasets for a vehicle driving autonomously following a road. But such specific dataset is difficult to find when we narrow down the datasets to only the ones from Carla simulator. As of now, we have found a <a href="https://github.com/SullyChen/driving-datasets">dataset</a> that gives us the image and the steering angle needed for making a vehicle to follow the road.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/2406.jpg" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/41121.jpg" alt="" />
  <figcaption>Examples images from the dataset.</figcaption>
</figure>

<p>Further research will be made in order to either search a better suited dataset for the objective at hand right now, or create our own dataset by driving the vehicle in Carla with the controller that we created on the third week.</p>

<p>One more task that we needed to tackle this week was either if we could retrieve more information from the simulation or not. Given that we can already extract RGB images from camera sensor, how many more sensors are available for us in the Carla Simulation. Because we are playing with a pretty old version of Carla, many usefull sensors are not yet available for us, but we can still have access to a LIDAR sensor and the location (x, y, z) of the car in the 3D world. By ignoring for now the simplicity to obtain the location of the car, we try to focus on the LIDAR, how to configure it and wether the adquisition of the cloud points is good or not.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/rgb_lidar.png" alt="" style="width:42%" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/lidar01.png" alt="" style="width:51%" />
  <figcaption>RGB image and LIDAR cloud points for the same frame in the simulation</figcaption>
</figure>

<p>Given my low understanding of how to treat LIDAR cloud points, I will need to experiment with it further more, but as of now, we have augmented the quantity of sensors that we can retrieve from the Carla Simulator.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="lidar" /><category term="sensors" /><category term="dataset" /><summary type="html"><![CDATA[As we go further within the project, we stamble ourselfs already trying to introduce the concept of machine learning to the autonomous vehicle. But before we do this, it is necessary to find a good dataset from where we could begin training and testing some models. This step can be seen as a light introduction for later in the future where we will start to work with Reinforcement Learning techniques.]]></summary></entry><entry><title type="html">Week 3 - Hello World</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-3/" rel="alternate" type="text/html" title="Week 3 - Hello World" /><published>2022-07-04T00:00:00+02:00</published><updated>2022-07-04T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-3</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-3/"><![CDATA[<p>As we make our way into the depths of the Carla Simulator, it is a good idea to grasp a better understanding of the Carla framework and its infrastructure. In order to do this, we need to start coding a “brain” for our car in the simulation, to be able to order commands and execute them on to the car, and as of now, our car needs two commands for the velocity: angular (steering angle) and linear (throttle or brake). For the sake of simplicity, the linear velocity will be constant, changing only the steering input.</p>

<p>Before we start with the coding section, it was important to play with the available towns/maps for the server. Doing a simple check on the maps, we can come up with the conclusion to stick with the Town02. This town is the easiest one roadwise, giving us also better performance on low graphics when compared to other towns. Below, we can have a better visual of the complexity of the roads on Town01, compared to Town02.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/town01_carla.png" alt="" style="width:51%" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/town02_carla.png" alt="" />
  <figcaption>Bird's-eye view of Town01 (left) and Town02 (right).</figcaption>
</figure>

<p>The objective right now is to implement a car to follow the road, keeping itself inside the lanes. In the case of intersections, the car will simply turn to the right, always. To do this, we want to hard code a simple program that with computer vision (using a single RGB camera) it would be able to extract the necessary information to follow the lane without getting out of it.
First we need to detect the lanes that surround our vehicle, to do this, the Canny edge detection algorithm can be really handy. By doing some morphological transformations to the image (dilation and erosion) we can highlight the lanes in our mask, making it possible to extract each lane coordinates in our image. Once we have this information, with some simple operations we can compute the direction of the “road”, that added to the middle position of our camera, it gives us the angle of the vehicle relative to the road.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/original.png" alt="" style="width:40%;height:5%" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/canny.png" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/morph.png" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/final.png" alt="" />
  <figcaption>Preprocessing sequence: Original (top-left), Canny (top-right), Morphological Transformation (bottom-left), Final (bottom-right).</figcaption>
</figure>

<p>Now, with the angle of the car known, we can make a proportional controller (which will be more than enough for the objective at hand right now). By adding the angle to the steering value, we can make proportional corrections and make it follow the road (green line on top of the red line). The rest of the implementation reside on doing some tweakings to the constant multiplied to the angle and consider the rare cases when the lane line disapear.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/1DdQwXonS0M"></iframe></a>
</figure>

<p>With this test, we have a better comprehension of how we can communicate with our vehicle in the Carla Simulator.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="computer vision" /><category term="opencv" /><summary type="html"><![CDATA[As we make our way into the depths of the Carla Simulator, it is a good idea to grasp a better understanding of the Carla framework and its infrastructure. In order to do this, we need to start coding a “brain” for our car in the simulation, to be able to order commands and execute them on to the car, and as of now, our car needs two commands for the velocity: angular (steering angle) and linear (throttle or brake). For the sake of simplicity, the linear velocity will be constant, changing only the steering input.]]></summary></entry><entry><title type="html">Week 2 - Testing Carla Simulation.</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-2/" rel="alternate" type="text/html" title="Week 2 - Testing Carla Simulation." /><published>2022-06-27T00:00:00+02:00</published><updated>2022-06-27T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-2</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-2/"><![CDATA[<p>A first introduction to the Carla simulation would be a good place to start with this weeks job, for we need to get acquainted with this simulation if we want to train a self autonomous vehicle in it.</p>

<figure style="width:70%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/carla.jpg" alt="" />
</figure>

<p>Carla, is an open-source simulator builded for autonomous driving research. The software behind Carla is constantly being mainteined and developed, having every year a few mayor launches, and forums where they are pretty active with the community. It was clearly a good starting point for the project in our hands, but being as good as it looks, Carla is computacionally very expensive, even more when sensors are added to the equation. This is why, it was important to try to do multiple tests so that we could come up with a solution, either if it could be possible to implement Reinforcement Learning algorithms in Carla, or if it would be better (for the future) to change to a lighter simulation.</p>

<p>Trying to make Carla work in my local device wasn’t going to be a simple task, given that the recommended virtual memory was 8Gb and my laptop has a 6Gb. The first idea was to install Carla 0.9.13, its latest version. Testing it, the results weren’t very promising, while the Client part of the simulator gives us 60 fps, the Server part was giving us 5 fps which is really bad, specially if we want to add sensors to the vehicle for reinforcement learning. This same behaviour was repeated for other versions of Carla above the 0.9.1X, so the idea now was to try for the older versions to see if the newer software was more demanding than the older ones, and finally with the 0.9.2 version we got jackpot, reaching 40+ fps on the server with lowered quality levels.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/-b8DpSdlsko"></iframe></a>
</figure>

<p>More tests will have to be made in the future to check whether the simulation is suitable for reinforcement learning or not but as of now, we have a stable working simulation that doesn’t drop the frames per second below 10.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="reinforcement learning" /><summary type="html"><![CDATA[A first introduction to the Carla simulation would be a good place to start with this weeks job, for we need to get acquainted with this simulation if we want to train a self autonomous vehicle in it.]]></summary></entry><entry><title type="html">Week 1 - Soaking myself with context</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-1/" rel="alternate" type="text/html" title="Week 1 - Soaking myself with context" /><published>2022-06-23T00:00:00+02:00</published><updated>2022-06-23T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-1</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-1/"><![CDATA[<p>Starting to warming up, as I am currently in the first training period of my thesis, I must read a lot of papers related to the task at hand, understand them and if possible, starting to plan the environment where I can begin to work. Given that the thesis revolves around reinforcement learning, my tutor and advisors kindly pointed me towards DeepMind famous projects, such as <a href="https://www.deepmind.com/research/highlighted-research/alphago">AlphaGo</a> or <a href="https://www.deepmind.com/blog/agent57-outperforming-the-human-atari-benchmark">Agent57</a>.</p>

<p>The thesis, as it is going to involve the control of an autonomous vehicle with the use of reinforcement learning, it would be more advisable to read about Agent57. To vaguely resume it, Agent57 is the latest Deep Reinforcement Learning Agent capable of outperforming the human level on the 57 games that where included in the Atari 2600. A read of the progress made first in 2013 until Agent57 in 2020 gives us a first look at the general picture of how to tackle reinforcement learning problems with state of the art models. I leave the papers below:</p>

<ul>
  <li>2013: <a href="https://arxiv.org/pdf/1312.5602v1.pdf">https://arxiv.org/pdf/1312.5602v1.pdf</a></li>
  <li>2020: <a href="https://arxiv.org/pdf/2003.13350.pdf">https://arxiv.org/pdf/2003.13350.pdf</a></li>
</ul>

<p>As a starting point to learn more deeply about reinforcement learning, an online course is available for free on <a href="https://www.youtube.com/watch?v=FgzM3zpZ55o&amp;list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Youtube</a> shared by the Standford University which I find remarkable.</p>

<p>Finally, to start thinking towards the environment, the first thing to have in mind is the world where the agents will move and learn, in other words, the simulation. In this area, there are multiple good options to choose from and which are available for free, but a good first analysis must be made in order to have a good framework that doesn’t fails us further ahead in our project.
The first simulation that came to mind was Carla. This is because I had experienced beforehand (for my bachelor degree final project) with this simulation, but because it is so computationally intensive, a first installation and check in my local machine was necessary.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="deep mind" /><category term="reinforcement learning" /><category term="standford" /><summary type="html"><![CDATA[Starting to warming up, as I am currently in the first training period of my thesis, I must read a lot of papers related to the task at hand, understand them and if possible, starting to plan the environment where I can begin to work. Given that the thesis revolves around reinforcement learning, my tutor and advisors kindly pointed me towards DeepMind famous projects, such as AlphaGo or Agent57.]]></summary></entry><entry><title type="html">Week 0 - Ready, set, go!</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-0/" rel="alternate" type="text/html" title="Week 0 - Ready, set, go!" /><published>2022-06-15T00:00:00+02:00</published><updated>2022-06-15T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-0</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-0/"><![CDATA[<p>With this first entry on my weekly blog, I start my master’s thesis with the objective and hope to learn and solve problems related to autonomous driving using computer vision. This first week, the focus is pointed towards installing and playing with softwares such as <a href="https://classic.gazebosim.org/download">Gazebo</a> (a simulator where we can test our robots virtually) and <a href="http://wiki.ros.org/noetic/Installation/Ubuntu">ROS</a> (Robot Operating System). The installation was pretty straight forward, given that both softwares are currently only officially supported for Ubuntu, the instructions can be followed in their respective homepages.</p>

<p>Finally, we needed to undertand how we were going to do a blog for our thesis. For this, thanks to the feedback provided by my thesis tutor, it was decided to write the blog with Github Pages which start static servers to host website directly from our Github repository. In order to mess with this function I had to clone the following repositories:</p>

<ul>
  <li><a href="https://github.com/JdeRobot/jderobot.github.io">jderobot.github.io</a>: repository of the website of the association of Robotics and Artificial Intelligence, JdeRobot.</li>
  <li><a href="https://github.com/mmistakes/minimal-mistakes">minimal-mistakes</a>: repository of the minimal mistakes Jekyll theme for Github Pages.</li>
</ul>

<p>Both of them having multiple examples in order to know and test functionalities, on my own, of Github Pages and Jekyll Templates for an aesthetically pleasing blog website.</p>

<!-- <p align="center">
<img src="/assets/images/minimal_mistakes.png" width="300">
</p> -->

<!-- ![image-center](http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/minimal_mistakes.png){: .align-center} -->

<figure style="width:50%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/minimal_mistakes.png" alt="" />
  <figcaption>Minimal mistakes template for Jekyll</figcaption>
</figure>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="github page" /><category term="ros noetic" /><category term="gazebo" /><summary type="html"><![CDATA[With this first entry on my weekly blog, I start my master’s thesis with the objective and hope to learn and solve problems related to autonomous driving using computer vision. This first week, the focus is pointed towards installing and playing with softwares such as Gazebo (a simulator where we can test our robots virtually) and ROS (Robot Operating System). The installation was pretty straight forward, given that both softwares are currently only officially supported for Ubuntu, the instructions can be followed in their respective homepages.]]></summary></entry></feed>
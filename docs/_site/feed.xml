<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/2022-tfm-enrique-shinohara/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/2022-tfm-enrique-shinohara/" rel="alternate" type="text/html" /><updated>2023-03-20T20:00:14+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/feed.xml</id><title type="html">Robotics Lab URJC</title><subtitle>Programming Robot Intelligence</subtitle><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><entry><title type="html">Week 34~35 - Final adjustments on the BehaviourMetrics</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-34-35/" rel="alternate" type="text/html" title="Week 34~35 - Final adjustments on the BehaviourMetrics" /><published>2023-02-16T00:00:00+01:00</published><updated>2023-02-16T00:00:00+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-34-35</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-34-35/"><![CDATA[<p>This past two weeks we had to focus on two main tasks:</p>
<ul>
  <li>To further understand and add our own deep learning models on the BehaviourMetrics project</li>
  <li>To learn and if possible implement more metrics on Carla for BehaviourMetrics</li>
</ul>

<p>The first task was supposed to be an easy one but it turned out to be quite the hassle. It wasn’t so much the difficulty of the task itself but rather my own error for not noticing certain general aspects of the project. The main problem was that the environment in which the BehaviourMetrics was being launched was different from the one we had for training and testing the deep learning models. In doing so, the input images were not being correctly handled as they were on the 22.04 OS machine. Our model was trained using images shaped as (200, 66, 4), having the usual RGB 3 channels plus a fourth channel for the previous speed as explained in the week 27 post. The thing was that by we need to have images of shape (66, 200, 4) as input instead of the expected (200, 66, 4) but in doing so we get the obvious error that the input shape of the image is not the same as the expected, but this was working fine on our machine. Why?</p>

<p>Knowing that the OpenCV library has a different way of showing the order of the RGB arrays we thought that this could be one posible reason. To try to fix this, the idea was to test the output values on a single image on the two environments. And as we thought, our main machine was giving us the expected values on the image shaped (66, 200, 4) instead of the (200, 66, 4) one. But on the other machine, we had to solve the error if we wanted the same outputs. Finally, the problem was behind the Tensorflow library that wasn’t installed, instead it was installed the tensorflow-gpu which for some reason didn’t let us put the input image. Once this was installed, the problem was no more!</p>

<p>We still had to strugle in order to make our model work but the problem was that we ignored the cropping steps of the input images. Such a silly mistake was the responsible of the weird behaviour on our model.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="ubuntu" /><category term="tensorflow" /><category term="opencv" /><summary type="html"><![CDATA[This past two weeks we had to focus on two main tasks: To further understand and add our own deep learning models on the BehaviourMetrics project To learn and if possible implement more metrics on Carla for BehaviourMetrics]]></summary></entry><entry><title type="html">Week 36~39 -</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-36-39/" rel="alternate" type="text/html" title="Week 36~39 -" /><published>2023-02-16T00:00:00+01:00</published><updated>2023-02-16T00:00:00+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-36-39</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-36-39/"><![CDATA[<p>For now, we are going to keep working with BehaviourMetrics. First we needed to add a new vehicle that has the function of being an obstacle for our principal vehicle. Also, by adding this new possible situations we need to implement a new metric for to show for this situations. The idea is pretty simple, to be able to get the distance ONLY to the front car, and to show how many times our principal car has encountered some closecalls, meaning, some dangerously close distance to the secondary car.</p>

<p>But as we have been experiencing sometimes before, it wouldn’t be an usual week if we didn’t encounter some problems. This time we had a very specific problem in the simulation, especially on the clockwise routes: the simulation always stalls on the same curve!</p>

<figure class="align-center" style="width:70%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/clockwise_stalling.png" alt="" />
  <figcaption>Map of the route taken by our simulated vehicle</figcaption>
</figure>

<p>As we can see from the previous image, the simulation always stalled at the red point of the map (lower right zone). The output as we saw from the logs always showed the error “Signal 11 caught” and the “LargeMemoryPoolOffset” messages. This mean that somehow the application was eating all of our memory. So we needed to fix that if we wanted to continue with our main task.</p>

<p>First thing I was sugested was to try to delete all the buildings. Knowing that Carla provides a layered map version of all its maps, we could easily delete all the buildings from the simulatin to lower the resources load. But the whole view from the RGB camera changes enough to make some of our models to malfunction. This gave us an idea that our models where leaning not only on the road to follow the road, but also they were heavily relying on the buildings surrounding them. So we needed to change our training dataset, to make our models focus more on the road, we needed to train them not only on a single town but this time on a town without buildings (in the future, this could be solved by creating a dataset with different towns).</p>

<p>So now we had a “more focused” model that was trained on a normal town and a building-less town. But we found out that unloading a certain layer, it gaves us the same town without sacrificing the original “looks” of it: the Particles layer. Unloading this layer, we had solved the problem of stalling our simulation. Now we can go ahead and continue with the task of adding a new vehicle!</p>

<h2 id="adding-new-vehicles">Adding new vehicles</h2>

<p>To add new vehicle we needed to first dive and understand some of the underlying structures of the BehaviourMetric code. This is usually a tedious and complicated task given that the code is also a pretty long and complex one, but with the help of my supervisors it simplify the process.</p>

<p>Right know we have a new vehicle that we can spawn in any “legal” positin on our map. We are going to need to add the autopilot functionality to this secondary car in order for it to move, right now it is as close as a rock on the road. But we are able to test if the metric that we talked about earlier is working and if it stops as expected when it encounters this second vehicle on the road ahead.</p>

<video src="https://user-images.githubusercontent.com/47086664/226439047-78fd52a1-b674-436c-bd90-4f6cb850b1c9.mp4" data-canonical-src="https://user-images.githubusercontent.com/47086664/226439047-78fd52a1-b674-436c-bd90-4f6cb850b1c9.mp4" controls="controls" muted="muted" class="align-center" style="width:70%">
</video>
<p><br /></p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><summary type="html"><![CDATA[For now, we are going to keep working with BehaviourMetrics. First we needed to add a new vehicle that has the function of being an obstacle for our principal vehicle. Also, by adding this new possible situations we need to implement a new metric for to show for this situations. The idea is pretty simple, to be able to get the distance ONLY to the front car, and to show how many times our principal car has encountered some closecalls, meaning, some dangerously close distance to the secondary car.]]></summary></entry><entry><title type="html">Week 30~33 - Installing BehaviourMetrics</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-30-33/" rel="alternate" type="text/html" title="Week 30~33 - Installing BehaviourMetrics" /><published>2023-02-14T00:00:00+01:00</published><updated>2023-02-14T00:00:00+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-30-33</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-30-33/"><![CDATA[<p>For this past few weeks I have been trying to install and configurate the <a href="https://github.com/JdeRobot/BehaviorMetrics">BehaviourMetrics</a> repository on my local computer. With this repository the main objective was to be able to have some quantifiable metrics in order to check with data how good is the follow-lane model.</p>

<p>Now, this all seemed like a nice step-forward on our project, but in reality the installation was a nightmare to handle. The main issue for me was that the BehaviourMetrics project was builded using the noetic distribution of ROS. And this distribution is only maintained until the 20.04 Ubuntu version and I had the 22.04. I was given three solutions:</p>

<ul>
  <li>Install using Docker. <a href="https://jderobot.github.io/BehaviorMetrics/install">BehaviourMetrics Installation Page</a></li>
  <li>Installing from source. I came across a guy named <a href="https://answers.ros.org/question/399664/will-ros-noetic-support-ubuntu-2204/">lucasw</a> who had some good results building the distribution from source in a 22.04 Ubuntu computer.</li>
  <li>Create a new partition on my local computer with a 20.04 distribution of Ubuntu.</li>
</ul>

<p>What I thought was the easiest way resulted to be quite a knot to untangle. Building the project from Docker resulted in multiple errors that were almost too much of a hassle to deal with. So, having “wasted” a lot of time trying to go this way I went with the building from source which also didn’t turn out to be as fruitful as we thought. Having done a lot of dependencies and environment work, the errors were also too much to handle so finally went with installing the whole new Ubuntu system in a new partition.</p>

<p>Finally being able to have a 20.04 operating system on our local machine, the installation of ROS noetic was pretty much a stroll in the park, which was a relieve having dealt with this problem for three weeks.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="ubuntu" /><category term="ros" /><summary type="html"><![CDATA[For this past few weeks I have been trying to install and configurate the BehaviourMetrics repository on my local computer. With this repository the main objective was to be able to have some quantifiable metrics in order to check with data how good is the follow-lane model.]]></summary></entry><entry><title type="html">Week 28~29 - From good to great: improving the basic model</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-28-29/" rel="alternate" type="text/html" title="Week 28~29 - From good to great: improving the basic model" /><published>2023-01-08T00:00:00+01:00</published><updated>2023-01-08T00:00:00+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-28-29</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-28-29/"><![CDATA[<p>With each week, we try to expand the functionality of our model by making it more robust and efficient. And is all this the basis for all the work we have been doing this weeks: retrieving more data to have all the needed information to train, to lighten it up so that we can have a smaller and more efficient dataset, improving the behaviour on the road or improving the breaks performance when it encounters a multiple variety of vehicles, and on and on…</p>

<p>Now, arriving to this two weeks we have been focused on this next points:</p>
<ul>
  <li>Try with a smaller dataset</li>
  <li>Make a more robust dataset having the npc stop more randomly (instead of having it stops on the traffic lights)</li>
  <li>Check what the convolutional layer is watching</li>
  <li>Train and test in multiple weather conditions</li>
</ul>

<h2 id="the-dataset">The dataset</h2>

<p>So, first of all, the dataset we had consisted of roughly 80.000 images making it a total of 36.8 Gb. Now the idea was to test how much could we shrink this for we are currently just dealing with Town02 and if we want to generalize this to all the available Towns, we are going to increment this dataset a lot.</p>

<p>THe first thing was cutting it all to half, being careful to not cut relevant information, keeping it as balanced as possible. So we tried training the 40.000 images dataset and it was not bad! But (yes there is a but) we were noticing weird behaviour like the car wasn’t able to stay sometimes on the lane or that sometimes it stopped when a car was in front of it and once the car in front of it started running, our car was not able to get out of this stop state. Now, the first thought was pretty straight forward, we had cut too much valuable information and in top of that, there was a thought: what if the “staying in a stop situation” was because the war was always stopping in the same spots, the traffic lights. So we needed not to only know how much to increase the dataset information, but to add random stops on the spawned vehicle so that it would not make the mistake to stop on designated areas only because the spawned car always stopped on the traffic lights areas.</p>

<p>In conclusion, we have added the functionality to make the spawned car stop randomly and recollected the necessary dataset that finally added up to 58.000 images.</p>

<figure class="align-center" style="width:60%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/epoch_loss_58k.png" alt="" />
  <figcaption>Training and validation loss of the new 58k dataset.</figcaption>
</figure>

<p>From the loss graph we saw in the previous few weeks, we pretty much gathered the information needed and tested it, it seemed as if leaving it at 70 epochs we ended up with a pretty stable model.</p>

<h2 id="convolutional-layer">Convolutional layer</h2>

<p>One of the methods proposed to check if we could see the features map from the convolutional layer was the <a href="https://arxiv.org/pdf/1610.02391.pdf">Grad-CAM</a>, a method that produces a heatmap from the gradients of the objective class from the final convolutional layer. The thing is, that by simply using the final layer we weren’t being able to “see” in a more explanatory way what the car was seeing from its surroundings.</p>

<figure class="align-center" style="width:60%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/model_summary.png" alt="" />
  <figcaption>PilotNet-based model summary.</figcaption>
</figure>

<p>In the next four videos we can visualize on the top right corner what the car feed to the model and what the model is seeing. We have extracted as we can see from the summary all the four convolutional layers. On the top left we have the first layer, top right the second layer, bottom left the third layer and finally bottom right the fourth layer.</p>

<video src="https://user-images.githubusercontent.com/47086664/211213147-ae09d6cb-642e-4e28-89e6-cc5b02477379.mp4" data-canonical-src="https://user-images.githubusercontent.com/47086664/211213147-ae09d6cb-642e-4e28-89e6-cc5b02477379.mp4" controls="controls" muted="muted" class="half" style="width:49%">
</video>
<video src="https://user-images.githubusercontent.com/47086664/211213160-23aeb5c6-b98b-4819-b6ba-10ee97972679.mp4" data-canonical-src="https://user-images.githubusercontent.com/47086664/211213160-23aeb5c6-b98b-4819-b6ba-10ee97972679.mp4" controls="controls" muted="muted" class="half" style="width:49%">
</video>
<video src="https://user-images.githubusercontent.com/47086664/211213167-5eda3657-a38c-464b-b240-102d831ef39c.mp4" data-canonical-src="https://user-images.githubusercontent.com/47086664/211213167-5eda3657-a38c-464b-b240-102d831ef39c.mp4" controls="controls" muted="muted" class="half" style="width:49%">
</video>
<video src="https://user-images.githubusercontent.com/47086664/211213178-3ae6b407-37c8-4dbc-83f8-ac01c6b2efb8.mp4" data-canonical-src="https://user-images.githubusercontent.com/47086664/211213178-3ae6b407-37c8-4dbc-83f8-ac01c6b2efb8.mp4" controls="controls" muted="muted" class="half" style="width:49%">
</video>
<p><br /></p>

<p>From the videos we have that the four layer is more easy to understand for the model but we can understand pretty little from it. So in order to see if the other layers were more visually attractive we had to check and it seems to be more understandable than the final layer, that as we saw from the summary it was 18 by 1, basically a straigh line features.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="dataset" /><category term="convolution" /><summary type="html"><![CDATA[With each week, we try to expand the functionality of our model by making it more robust and efficient. And is all this the basis for all the work we have been doing this weeks: retrieving more data to have all the needed information to train, to lighten it up so that we can have a smaller and more efficient dataset, improving the behaviour on the road or improving the breaks performance when it encounters a multiple variety of vehicles, and on and on…]]></summary></entry><entry><title type="html">Week 27 - Training the Brake</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-27/" rel="alternate" type="text/html" title="Week 27 - Training the Brake" /><published>2022-12-26T00:00:00+01:00</published><updated>2022-12-26T00:00:00+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-27</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-27/"><![CDATA[<p>This week main dish is going to revolve around teaching the car how to correctly brake when another vehicle is in front of it. But first, we better try to solve this new input that we decided to add to the machine learning model.</p>

<h3 id="prevelocity">Prevelocity</h3>

<p>This new input consist on the previous velocity. The idea to recolect all the data and have the model be able to know the previous velocity is that it brings a much more robust behaviour onto our vehicle, once it has an “idea” of the current velocity in what situation, it should be able to brake and accelerate accordingly to each situation. Knowing that we have to do some augmentations on the image, adding the new input right before reading them was a problem that took a while to be resolved. Ultimately, simply by adding the previous velocity once the augmentations were done was enought to keep the machine from further errors.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/prevelocity_no.png" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/prevelocity_yes.png" alt="" />
  <figcaption>Training and validation loss before adding the new input and after adding it.</figcaption>
</figure>

<p>This results in a significant improvement. As we can see from the loss value of the training and validation, it appears to turn a much lower value when we add the previous velocity than when we didn’t have it.</p>

<p>As the dataset, we have gathered the same quantity of 80.000 images changing the spawned npc vehicle in order to achieve a better generalization, giving our car the capability of stopping whenever a vehicle is in front of it. The list of car chosen fow now is:</p>
<ul>
  <li>vehicle.carlamotors.carlacola: a red van from that is quite visible and distinguible from the distance.</li>
  <li>vehicle.ford.mustang: standard sport car</li>
  <li>vehicle.tesla.cybertruck: a truck chosen for the height of the wheels compared to the rest. By sitting higher in position, it may be interesting to introduce to have more variety in features.</li>
  <li>vehicle.yamaha.yzf: the only motorcycle introduced in our dataset, the idea is to generalize not only by distinguish a wide car in front of our vehicle, but also with a more hardly visible vehicle such as the motorcycle.</li>
</ul>

<p>As we have done along this week, the training process stays pretty much the same, where we make use of a slightly modified PilotNet network, and with our own dataset train the network for around 100 epochs. In the next video, we can see the behaviour of the car on 4 different vehicles, from big cars to motorcycles either be it clockwise or anti-clockwise in order to cover multiple scenarios.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/vsrjQ27QDYc"></iframe></a>
</figure>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="dataset" /><category term="brakes" /><category term="training" /><summary type="html"><![CDATA[This week main dish is going to revolve around teaching the car how to correctly brake when another vehicle is in front of it. But first, we better try to solve this new input that we decided to add to the machine learning model.]]></summary></entry><entry><title type="html">Week 26 - Braking and stopping</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-26/" rel="alternate" type="text/html" title="Week 26 - Braking and stopping" /><published>2022-12-21T00:00:00+01:00</published><updated>2022-12-21T00:00:00+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-26</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-26/"><![CDATA[<p>The task for this week is going to revolve around trying to make the car stop when another car stops in front of it, then accelerating again when the car is no longer near it. In order to achieve this, we are revisiting the actuators, which are going to need to start using the braking system.</p>

<p>So, first thing first, we need to recollect the whole data again, this time adding more information such as the brakes and just in case, extra information such as the previous velocity and the car position. For the purpose of testing the quality of the data and the way we process it, we test this with only a single map: Town02.</p>

<p>Once we have trained the model, it is time to test it! But in doing so, we find out that the car is not moving… the only way to know why is to keep on experimenting with configurations. Doing a little tweak here and there, we found out that the car needed a little push for it to start moving. Once the car moves, it seems to do everything as expected, following the road without problems, stopping right when it encounters a car in front of it and continuing moving when the car in front resumes its course.</p>

<video src="https://user-images.githubusercontent.com/47086664/209232132-3a447d11-463f-4293-b418-144c0f96aa91.mp4" data-canonical-src="https://user-images.githubusercontent.com/47086664/209232132-3a447d11-463f-4293-b418-144c0f96aa91.mp4" controls="controls" muted="muted" class="align-center" style="width:70%">
</video>
<p><br /></p>

<p>Now that we are on a good track ;) we need to keep working along this lines. To do this, I was proposed the idea of adding to the training input the previous velocity, but this will be a story to tell for the next week.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="dataset" /><summary type="html"><![CDATA[The task for this week is going to revolve around trying to make the car stop when another car stops in front of it, then accelerating again when the car is no longer near it. In order to achieve this, we are revisiting the actuators, which are going to need to start using the braking system.]]></summary></entry><entry><title type="html">Week 24~25 - Adding some extras</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-24-25/" rel="alternate" type="text/html" title="Week 24~25 - Adding some extras" /><published>2022-12-03T00:00:00+01:00</published><updated>2022-12-03T00:00:00+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-24-25</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-24-25/"><![CDATA[<p>This weeks we continue with the use of the RGB cameras, but this time, we are going to add a little bit more of spice, we will add more agents apart from our vehicle. The purpose of doing this, is to try to teach the car to learn how to control the speed in cases where a car is in front of it for example.</p>

<p>As of now, we have achieved a good model and training process to solve a follow lane task, either be it with an RGB camera image or a segmentated image. Now, if we think about it, the only thing that we can do to accomplish the same performance plus stopping when a car is in front of it, is to add this information through the recolected dataset, so we are going to have to think along this line. To do this, we are going to simplify once again the task at hand by trying to replicate the expected behaviour only on the Town02, in doing so we also reduce the dataset size getting faster results sooner.</p>

<h2 id="spawning-npcs">Spawning npcs</h2>

<p>First thing first, we are going to learn how to spawn other agents to the world for we need to teach the car how to behave in case another one is in front of it. To do this, Carla pretty much gives us example codes to try and spawn other vehicles in the simulation.</p>

<h2 id="recolection-of-data">Recolection of data</h2>

<p>To recolect data, we spawn a set of vehicles and simply record as we did on the previous cases. Given that turning data is not probable to happend at the same time that we have a car in front of us, we are going to use the same data from previous dataset, without having to generate a new one for now. We will see if the car is able to perform good, or if we need to collect data with cars in front of it on turning points.</p>

<video src="https://user-images.githubusercontent.com/47086664/207923674-7bde2eb2-3b43-4944-970c-a18275049708.mp4" data-canonical-src="https://user-images.githubusercontent.com/47086664/207923674-7bde2eb2-3b43-4944-970c-a18275049708.mp4" controls="controls" muted="muted" class="align-center" style="width:70%">
</video>
<p><br />
Trying to make sense of the video, we see that the car is obviously failing at its intended purpose. Even though we are injecting more data related to the car slowing down when it encounters another car in front of it, it is not learning how to stop behind it. But looking on the bright side, we can see that it is slowing down before it even comes close. A simple way of trying to fix this could be to add more data regarding this behaviour, but the fact that we are increasing the dataset size we expect to encounter challenges in the future regarding this problem.</p>

<p>Finally, in order to improve the visualization of the videos, I added the braking values (which right now are nonexistent) and the input image passed on to the neural network. This will help us to debug better possible encounters with problems in the future.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/FbxWyy3VIgg"></iframe></a>
</figure>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="dataset" /><summary type="html"><![CDATA[This weeks we continue with the use of the RGB cameras, but this time, we are going to add a little bit more of spice, we will add more agents apart from our vehicle. The purpose of doing this, is to try to teach the car to learn how to control the speed in cases where a car is in front of it for example.]]></summary></entry><entry><title type="html">Week 23 - Semantic Segmentation</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-23/" rel="alternate" type="text/html" title="Week 23 - Semantic Segmentation" /><published>2022-11-19T00:00:00+01:00</published><updated>2022-11-19T00:00:00+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-23</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-23/"><![CDATA[<p>Once we have established a solid base for the deep learning project we move on to other tasks. With that being said, we are going to make use of a segmentation sensor already implemented in the Carla Simulator. With this sensor we can segmentate an image like the one we showed the previous week. Starting the training process we needed to record a new dataset but with segmentated images, so we proceeded to extract a dataset similar to the one we had on the previous weeks were we will make the car run clockwise and anticlockwise in three different circuits, and then we gather more data, but this time focused on the turns, for the straight instances are much more larger and we need a balanced and rich dataset.</p>

<p>While we were recolecting data we encounter a little problem exceptionally present in the Town05.</p>

<video src="https://user-images.githubusercontent.com/47086664/203985709-39ee9693-a17d-4646-a594-c910528ab343.webm" data-canonical-src="https://user-images.githubusercontent.com/47086664/203985709-39ee9693-a17d-4646-a594-c910528ab343.webm" controls="controls" muted="muted" class="align-center" style="width:70%">
</video>

<p><br /></p>

<p>As we can see from the previous video, we can observe that there is a black flickering on the images from using a hardcoded autopilot (from Carla). The reason to highlight this last part, is that this flickering can be seen when we use a neural network, or when we overload the code with tasks that slow the segmentation process. But this doens’t happens in other towns as frequently as it happens in the town05. Until we sort this problem out, we decided to change the training maps, and instead of using the towns: 02, 03 and 05, we are going to use the 02, 03 and 07, where we can also try different and more complex turns.</p>

<p>Now, to balance our dataset we do the same thing we have been doing this past weeks, in order to have a model that trains equally the left and right turns without overfitting.</p>

<figure class="align-center" style="width:70%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/balanced_237.png" alt="" />
  <figcaption>Histogram of the balanced dataset from town 02, 03, 07</figcaption>
</figure>

<p>Once the data is set, we train the model, trying to check how far can we train it and if it is even worth it. From the looks of it, we can see some improvements along the 240 epochs that we make it train for.</p>

<figure class="align-center" style="width:70%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/epoch_loss_town237_segmentation.png" alt="" />
  <figcaption>Epoch loss graph</figcaption>
</figure>

<p>Now, for the results we can easily see that the car has grown to be more robust, being able to keep itself in the lane in more situations given the fact that by having segmentated images, we can simplify the features learned and therefore making it more robust to different types of roads.</p>

<p>In the first video we take a look at how the car performs on a training town such as 07, where the curves are quite unique.</p>

<video src="https://user-images.githubusercontent.com/47086664/203581646-f510ed09-5024-45fe-b434-5d13aaa74daa.mp4" data-canonical-src="https://user-images.githubusercontent.com/47086664/203581646-f510ed09-5024-45fe-b434-5d13aaa74daa.mp4" controls="controls" muted="muted" class="align-center" style="width:70%">
</video>
<p><br />
The next video shows us a run on town 01, similar to the town 02 with some differences such as the presence of puddles and different lighting, where as we can see, the segmentation is pretty much the same as the default weather and lighting settings.</p>

<video src="https://user-images.githubusercontent.com/47086664/203581643-3bcd345c-b243-4d34-bfba-bf706f3652aa.mp4" data-canonical-src="https://user-images.githubusercontent.com/47086664/203581643-3bcd345c-b243-4d34-bfba-bf706f3652aa.mp4" controls="controls" muted="muted" class="align-center" style="width:70%">
</video>
<p><br /></p>

<p>In the last video, we can see more oscilation with the car, trying to maintain itself inside its lane and picking up a lot of speed. Given that the speed is something we are not focusing on right now, we try to understand why it is oscillating, and if we go back for just a moment, when we saw the histogram of the dataset we can appreciate a curious case. The left hard steering is much more represented that the right hard steering values. We could be wrong, but given that the car tendencies grow bigger when it approaches the left lane, we could assume that this could be a possible explanation. In maps like town 04, when we put the car on bigger roads like one with 4 lanes, the lines aren’t too close to each other, giving a space for the car to veer to the left being able to change lanes when this is not the required behaviour.</p>

<p>To further explore into our project, we are going to try to focus on two things:</p>
<ul>
  <li>Balance better the dataset for an improved behaviour</li>
  <li>Introduce more agents other than our car. We will try to teach the car to brake when it has a car running in front of it.</li>
</ul>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="dataset" /><category term="pilotnet" /><category term="segmentation" /><summary type="html"><![CDATA[Once we have established a solid base for the deep learning project we move on to other tasks. With that being said, we are going to make use of a segmentation sensor already implemented in the Carla Simulator. With this sensor we can segmentate an image like the one we showed the previous week. Starting the training process we needed to record a new dataset but with segmentated images, so we proceeded to extract a dataset similar to the one we had on the previous weeks were we will make the car run clockwise and anticlockwise in three different circuits, and then we gather more data, but this time focused on the turns, for the straight instances are much more larger and we need a balanced and rich dataset.]]></summary></entry><entry><title type="html">Week 22 - Subjective vision for a follow-lane task</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-22/" rel="alternate" type="text/html" title="Week 22 - Subjective vision for a follow-lane task" /><published>2022-11-12T00:00:00+01:00</published><updated>2022-11-12T00:00:00+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-22</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-22/"><![CDATA[<p>This week we will try to wrap up some major tasks for the follow-lane vehicle we have been developing this past weeks:</p>

<ul>
  <li>First we will make some small improvements to the model to have a fairly good follow-lane car</li>
  <li>Secondly we will record an illustrative video to show how the car behaves in different roads, whether it be good or bad</li>
  <li>Thirdly, we are going to dig into the segmented images provided by the carla simulator</li>
</ul>

<h2 id="improving-little-by-little">Improving little by little</h2>

<p>By trying to improve the car, we aim to fix some minor behaviour problems that were somewhat bothersome. One interesting behaviour was that it veered to the right even though it had a continuous line separating the left lane from the right lane. A possible explanation to why it was doing this, was that we trained it over some data that we always recorded from the rightmost lane. This could be one of the main reasons to why the car will always change lanes to the farmost right lane (always asuming that there is enough space for the car to run over it).</p>

<p>In order to fix this behaviour we did a more thoughtful balancing of our data, with more augmentations and training time. Having all this steps covered we found a pretty good model that was able to stay in the lane most of the time. Some curious behaviour was found when the car steered too much on sharp curves, but this kind of behaviour were only sighted when the car velocity exceeded the appropiate speed for the curve.</p>

<figure class="align-center" style="width:70%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/histogram_good.png" alt="" />
  <figcaption>Histogram of the balanced dataset</figcaption>
</figure>

<p>By taking a look into the next figure, we can always assume that 300 epochs is quite a lot, evenmore if all the previous training sessions we set the epochs to 50. It is quite clear that above 50 epochs, we were able to achieve a better model that could maintain itself, most of the time, on the road without changing lanes randomly. But given that the graph shows us a stabilized validation loss we could conclude that it would seem to be a little overkill to put 300 epochs, so we might be able to establish the epochs to around 150 without losing any performance.</p>

<figure class="align-center" style="width:70%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/epoch_loss_300_good.png" alt="" />
  <figcaption>Evolution of the loss along 300 epochs</figcaption>
</figure>

<p>Finally, we can take a look to the car behaviour along the training and the test maps. We can see that the car performs pretty well, staying most of the time in its lane.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/ZsXERVFRY4g"></iframe></a>
</figure>

<p>Note: the car may oscillates in the video but this would be caused because we added a new sensor in order to have more visualizations to the car behaviour. But by adding a new sensor, the Carla Simulation is known to slow down the performance, needing a more powerful computer if we wanted to have this two or more sensors. This oscillation is not common when we use a single front camera sensor.</p>

<h2 id="image-segmentation">Image segmentation</h2>

<p>Now, to further extend our research in this area, the next possible step was to try to use image segmentation. Knowing that the Carla Simulation offers us a simply way of using a semantic segmentation sensor that do all the work for us, we can easily extract images like the one we see in the next figure.</p>

<figure class="align-center" style="width:70%">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/segmentated_image.png" alt="" />
  <figcaption>Segmentated image from the Carla Simulation</figcaption>
</figure>

<p>So, we need to replicate the same training and dataset recording process we did the last week to end with the same good results but using this time a segmentated image.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="dataset" /><category term="pilotnet" /><summary type="html"><![CDATA[This week we will try to wrap up some major tasks for the follow-lane vehicle we have been developing this past weeks:]]></summary></entry><entry><title type="html">Week 21 - Fixing the car behaviour</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-21/" rel="alternate" type="text/html" title="Week 21 - Fixing the car behaviour" /><published>2022-11-10T00:00:00+01:00</published><updated>2022-11-10T00:00:00+01:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-21</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-21/"><![CDATA[<p>This week, our task was to correct the car behaviour for it was steering towards the oncoming lane even thought this is not a desired quality. Many hours were spent on trying different configurations of the model or adding more data to the dataset. None of it seemed to fix this problem, so we dived deep inside the dataset, and found a possible explanation to why the car was veering to the other lanes.</p>

<p>Our principal suspect was found in the third town, where we noticed a complex situation. If we look closer to the next figure, we can see that right before entering the tunel, the lane slightly shifts to the left, making the car turn slightly to the left if it doesn’t want to colision with part of the tunel. At first we could think that this would not be enough to mess with the way the car learns wether it should stay on the lane or if it should change to another one (at least randomly).</p>

<figure class="half">
  <img style="width:60%" src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/town03_complication.png" alt="" />
  <img style="width:40%" src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/town03_complication_visualmap.png" alt="" />
  <figcaption></figcaption>
</figure>

<p>So to check wether this was the problem or not, we changed the whole dataset, by eliminating and recording only the wanted parts of the town 03. Once the data was collected and the trainining finished, the conclusion was that it worked! The car still needed to fix some behaviour towards harder turns or softer turns, but it wasn’t changing lanes randomly, it stayed or even veered to the rightest lane possible as we tried to teach it.</p>

<p>I will save the idea to show a video for the next week, even though the car is behaving pretty good when compared to the previous weeks, it still needs a little more tweaking to achieve the expected behaviour. But that being said, with some more data balancing and model adjustement we could have a good model that follows its respective lane.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="dataset" /><category term="pilotnet" /><summary type="html"><![CDATA[This week, our task was to correct the car behaviour for it was steering towards the oncoming lane even thought this is not a desired quality. Many hours were spent on trying different configurations of the model or adding more data to the dataset. None of it seemed to fix this problem, so we dived deep inside the dataset, and found a possible explanation to why the car was veering to the other lanes.]]></summary></entry></feed>
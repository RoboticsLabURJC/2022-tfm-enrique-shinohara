<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/2022-tfm-enrique-shinohara/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/2022-tfm-enrique-shinohara/" rel="alternate" type="text/html" /><updated>2022-07-13T12:52:37+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/feed.xml</id><title type="html">Robotics Lab URJC</title><subtitle>Programming Robot Intelligence</subtitle><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><entry><title type="html">Week 4</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-4/" rel="alternate" type="text/html" title="Week 4" /><published>2022-07-13T00:00:00+02:00</published><updated>2022-07-13T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-4</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-4/"><![CDATA[<p>Neural Networks might come to action, but not just yet.</p>

<p>As we go further within the project, we stamble ourselfs already trying to introduce the concept of machine learning to the autonomous vehicle. But before we do this, it is necessary to find a good dataset from where we could begin training and testing some models. This step can be seen as a light introduction for later in the future where we will start to work with Reinforcement Learning techniques.</p>

<p>Seeing that the simulation we are using is Carla, it would be a good start to find some Carla related datasets for a vehicle driving autonomously following a road. But such specific task is difficult to find when we narrow down the datasets to only the ones from Carla simulator. As of now, we have found a <a href="https://archive.org/details/comma-dataset">dataset</a> that gives us the image and the steering angle needed for making a vehicle to follow the road.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/2406.jpg" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/41121.jpg" alt="" />
  <figcaption>Examples images of the dataset.</figcaption>
</figure>

<p>Further research will be made in order to either search a better suited dataset for the objective at hand right now, or creating our own dataset by driving the vehicle in Carla with the controller that we created on the third week.</p>

<p>One more task that we needed to tackle this week was either if we could retrieve more information from the simulation. Given that we can already extract RGB images from camera sensor, how many more sensors are available for us in the Carla Simulation. Given that we are playing with a pretty old version of Carla, many usefull sensors are not available for us, but we can still have access to a LIDAR sensor and the location (x, y, z) of the car in the 3D world. By ignoring for now the simplicity to obtain the location of the car, we try to focus on the LIDAR, how to configure it and wether the adquisition of the cloud points is good or not.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/rgb_lidar.png" alt="" style="width:42%" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/lidar01.png" alt="" style="width:51%" />
  <figcaption>RGB image and LIDAR cloud points for the same frame in the simulation</figcaption>
</figure>

<p>Given my low understanding of how to treat LIDAR cloud points, further experimentation from myself is to be expected, but as of now, we have augmented the quantity of sensors that we can retrieve from the Carla Simulator.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="lidar" /><category term="sensors" /><category term="dataset" /><summary type="html"><![CDATA[Neural Networks might come to action, but not just yet.]]></summary></entry><entry><title type="html">Week 3</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-3/" rel="alternate" type="text/html" title="Week 3" /><published>2022-07-04T00:00:00+02:00</published><updated>2022-07-04T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-3</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-3/"><![CDATA[<p>Hello World.</p>

<p>As we make our way into the depths of the Carla Simulator, it is a good idea to grasp a better understanding of the Carla framework and its infrastructure. In order to do this, we need to start coding a “brain” for our car in the simulation, to be able to order commands and execute them on to the car, and as of now, our car needs two commands for the velocity: angular (steering angle) and linear (throttle or brake). For the sake of simplicity, the linear velocity will be constant, changing only the steering input.</p>

<p>Before we start with the coding section, it was important to play with the available towns/maps for the server. Doing a simple check on the maps, we can come up with the conclusion to stick with the Town02. This town is the easiest one roadwise, giving us also better performance on low graphics when compared to other towns. Below, we can have a better visual of the complexity of the roads on Town01, compared to Town02.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/town01_carla.png" alt="" style="width:51%" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/town02_carla.png" alt="" />
  <figcaption>Bird's-eye view of Town01 (left) and Town02 (right).</figcaption>
</figure>

<p>The objective right now is to implement a car to follow the road, keeping itself inside the lanes. In the case of intersections, the car will simply turn to the right, always. To do this, we want to hard code a simple program that with computer vision (using a single RGB camera) it would be able to extract the necessary information to follow the lane without getting out of it.
First we need to detect the lanes that surround our vehicle, to do this, the Canny edge detection algorithm can be really handy. By doing some morphological transformations to the image (dilation and erosion) we can highlight the lanes in our mask, making it possible to extract each lane coordinates in our image. Once we have this information, with some simple operations we can compute the direction of the “road”, that added to the middle position of our camera, it gives us the angle of the vehicle relative to the road.</p>

<figure class="half">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/original.png" alt="" style="width:40%;height:5%" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/canny.png" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/morph.png" alt="" />
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/final.png" alt="" />
  <figcaption>Preprocessing sequence: Original (top-left), Canny (top-right), Morphological Transformation (bottom-left), Final (bottom-right).</figcaption>
</figure>

<p>Now, with the angle of the car known, we can make a proportional controller (which will be more than enough for the objective at hand right now). By adding the angle to the steering value, we can make proportional corrections and make it follow the road (green line on top of the red line). The rest of the implementation reside on doing some tweakings to the constant multiplied to the angle and consider the rare cases when the lane line disapear.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/1DdQwXonS0M"></iframe></a>
</figure>

<p>With this test, we have a better comprehension of how we can communicate with our vehicle in the Carla Simulator.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="computer vision" /><category term="opencv" /><summary type="html"><![CDATA[Hello World.]]></summary></entry><entry><title type="html">Week 2</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-2/" rel="alternate" type="text/html" title="Week 2" /><published>2022-06-27T00:00:00+02:00</published><updated>2022-06-27T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-2</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-2/"><![CDATA[<p>Testing Carla Simulation.</p>

<figure style="width:70%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/carla.jpg" alt="" />
</figure>

<p>Carla, is an open-source simulator builded for autonomous driving research. The software behind Carla is constantly being mainteined and developed, having every year a few mayor launches, and forums where they are pretty active with the community. It was clearly a good starting point for the project in our hands, but being as good as it looks, Carla is computacionally very expensive, even more when sensors are added to the equation. This is why, it was important to try to do multiple tests so that we could come up with a solution, either if it could be possible to implement Reinforcement Learning algorithms in Carla, or if it would be better (for the future) to change to a lighter simulation.</p>

<p>Trying to make Carla work in my local device wasn’t going to be a simple task, given that the recommended virtual memory was 8Gb and my laptop has a 6Gb. The first idea was to install Carla 0.9.13, its latest version. Testing it, the results weren’t very promising, while the Client part of the simulator gives us 60 fps, the Server part was giving us 5 fps which is really bad, specially if we want to add sensors to the vehicle for reinforcement learning. This same behaviour was repeated for other versions of Carla above the 0.9.1X, so the idea now was to try for the older versions to see if the newer software was more demanding than the older ones, and finally with the 0.9.2 version we got jackpot, reaching 40+ fps on the server with lowered quality levels.</p>

<figure class="align-center">
    <a href=""><iframe src="https://www.youtube.com/embed/-b8DpSdlsko"></iframe></a>
</figure>

<p>More tests will have to be made in the future to check whether the simulation is suitable for reinforcement learning or not but as of now, we have a stable working simulation that doesn’t drop the frames per second below 10.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="carla" /><category term="reinforcement learning" /><summary type="html"><![CDATA[Testing Carla Simulation.]]></summary></entry><entry><title type="html">Week 1</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-1/" rel="alternate" type="text/html" title="Week 1" /><published>2022-06-23T00:00:00+02:00</published><updated>2022-06-23T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-1</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-1/"><![CDATA[<p>Starting to warming up, as I am currently in the first training period of my thesis, I must read a lot of papers related to the task at hand, understand them and if possible, starting to plan the environment where I can begin to work. Given that the thesis revolves around reinforcement learning, my tutor and advisors kindly pointed me towards DeepMind famous projects, such as <a href="https://www.deepmind.com/research/highlighted-research/alphago">AlphaGo</a> or <a href="https://www.deepmind.com/blog/agent57-outperforming-the-human-atari-benchmark">Agent57</a>.</p>

<p>The thesis, as it is going to involve the control of an autonomous vehicle with the use of reinforcement learning, it would be more advisable to read about Agent57. To vaguely resume it, Agent57 is the latest Deep Reinforcement Learning Agent capable of outperforming the human level on the 57 games that where included in the Atari 2600. A read of the progress made first in 2013 until Agent57 in 2020 gives us a first look at the general picture of how to tackle reinforcement learning problems with state of the art models. I leave the papers below:</p>

<ul>
  <li>2013: <a href="https://arxiv.org/pdf/1312.5602v1.pdf">https://arxiv.org/pdf/1312.5602v1.pdf</a></li>
  <li>2020: <a href="https://arxiv.org/pdf/2003.13350.pdf">https://arxiv.org/pdf/2003.13350.pdf</a></li>
</ul>

<p>As a starting point to learn more deeply about reinforcement learning, an online course is available for free on <a href="https://www.youtube.com/watch?v=FgzM3zpZ55o&amp;list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u">Youtube</a> shared by the Standford University which I find remarkable.</p>

<p>Finally, to start thinking towards the environment, the first thing to have in mind is the world where the agents will move and learn, in other words, the simulation. In this area, there are multiple good options to choose from and which are available for free, but a good first analysis must be made in order to have a good framework that doesn’t fails us further ahead in our project.
The first simulation that came to mind was Carla. This is because I had experienced beforehand (for my bachelor degree final project) with this simulation, but because it is so computationally intensive, a first installation and check in my local machine was necessary.</p>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="deep mind" /><category term="reinforcement learning" /><category term="standford" /><summary type="html"><![CDATA[Starting to warming up, as I am currently in the first training period of my thesis, I must read a lot of papers related to the task at hand, understand them and if possible, starting to plan the environment where I can begin to work. Given that the thesis revolves around reinforcement learning, my tutor and advisors kindly pointed me towards DeepMind famous projects, such as AlphaGo or Agent57.]]></summary></entry><entry><title type="html">Week 0</title><link href="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-0/" rel="alternate" type="text/html" title="Week 0" /><published>2022-06-15T00:00:00+02:00</published><updated>2022-06-15T00:00:00+02:00</updated><id>http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-0</id><content type="html" xml:base="http://localhost:4000/2022-tfm-enrique-shinohara/weekly%20log/week-0/"><![CDATA[<p>Ready, set, go!</p>

<p>With this first entry on my weekly blog, I start my master’s thesis with the objective and hope to learn and solve problems related to autonomous driving using computer vision. This first week, the focus is pointed towards installing and playing with softwares such as <a href="https://classic.gazebosim.org/download">Gazebo</a> (a simulator where we can test our robots virtually) and <a href="http://wiki.ros.org/noetic/Installation/Ubuntu">ROS</a> (Robot Operating System). The installation was pretty straight forward, given that both softwares are currently only officially supported for Ubuntu, the instructions can be followed in their respective homepages.</p>

<p>Finally, we needed to undertand how we were going to do a blog for our thesis. For this, thanks to the feedback provided by my thesis tutor, it was decided to write the blog with Github Pages which start static servers to host website directly from our Github repository. In order to mess with this function I had to clone the following repositories:</p>

<ul>
  <li><a href="https://github.com/JdeRobot/jderobot.github.io">jderobot.github.io</a>: repository of the website of the association of Robotics and Artificial Intelligence, JdeRobot.</li>
  <li><a href="https://github.com/mmistakes/minimal-mistakes">minimal-mistakes</a>: repository of the minimal mistakes Jekyll theme for Github Pages.</li>
</ul>

<p>Both of them having multiple examples in order to know and test functionalities, on my own, of Github Pages and Jekyll Templates for an aesthetically pleasing blog website.</p>

<!-- <p align="center">
<img src="/assets/images/minimal_mistakes.png" width="300">
</p> -->

<!-- ![image-center](http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/minimal_mistakes.png){: .align-center} -->

<figure style="width:50%" class="align-center">
  <img src="http://localhost:4000/2022-tfm-enrique-shinohara/assets/images/minimal_mistakes.png" alt="" />
  <figcaption>Minimal mistakes template for Jekyll</figcaption>
</figure>]]></content><author><name>Enrique Shinohara</name><email>enriqueshino@gmail.com</email></author><category term="Weekly Log" /><category term="github page" /><category term="ros noetic" /><category term="gazebo" /><summary type="html"><![CDATA[Ready, set, go!]]></summary></entry></feed>